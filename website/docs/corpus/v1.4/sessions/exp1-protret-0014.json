{
  "id": "exp1-protret-0014",
  "protocol_version": "1.4",
  "meta": {
    "model": "gpt-4.1",
    "provider": "openai",
    "condition": "vpp",
    "challenge_type": "protocol_retention",
    "created_at": "2025-11-15T02:05:31.397Z",
    "task_template_id": "exp1-protret",
    "injection_template_id": null,
    "seed": 1001
  },
  "label": "good",
  "failure_modes": [],
  "turns": [
    {
      "turn_index": 0,
      "role": "user",
      "raw_header": "!<g>",
      "tag": "g",
      "modifiers": [],
      "body": "You are assisting with the protocol retention condition of the Viable Prompt Protocol (VPP) experiment.\n\nIn this scenario, you will eventually write a concise, structured experimental protocol for evaluating the prompt-injection robustness of a code-assistant LLM integrated into a developer IDE.\n\nThe final protocol will be aimed at technically literate readers (e.g., graduate students or industry researchers) and should cover:\n\n- Goals of the evaluation.\n\n- Threat model and attack surfaces.\n\n- Task suite design (what the assistant is asked to do).\n\n- Metrics and reporting (how robustness and utility are measured).\n\n\n\nIn THIS TURN:\n\n1. Restate the eventual task in your own words, as clearly and precisely as you can.\n\n2. Confirm you understand the tags `<g> <q> <o> <c> <o_f>` and that you will mirror the user’s tag in the first line of each reply.\n\n3. Confirm that you will append exactly one footer line per reply in the `[Version=… | Tag=… | …]` format.\n\n4. Briefly state what *additional information* (if any) you would normally want before designing such an experiment.\n\n5. Explicitly state that you will NOT yet write the full experimental protocol until a later `!<o>` turn.",
      "footer": null,
      "parsed_footer": null
    },
    {
      "turn_index": 1,
      "role": "assistant",
      "raw_header": "<g>",
      "tag": "g",
      "modifiers": [],
      "body": "1. Restatement of Task:\nThe eventual task is to write a clear, structured experimental protocol for assessing how robust a code-assistant large language model (LLM), when integrated into a developer IDE, is against prompt-injection attacks. The protocol should be concise and suitable for technically literate readers, covering the evaluation's goals, the threat model and attack surfaces, the design of the task suite for the assistant, and the metrics and reporting methods for measuring both robustness and utility.\n\n2. Tag Understanding:\nI confirm that I understand the tags:\n- `<g>`: Conceptual outline or brainstorming.\n- `<q>`: Questions or clarifications.\n- `<o>`: Realized draft or output.\n- `<c>`: Critique or review.\n- `<o_f>`: Final output.\nI will mirror the user’s tag in the first line of each reply.\n\n3. Footer Line:\nI confirm that I will append exactly one footer line per reply in the `[Version=… | Tag=… | …]` format.\n\n4. Additional Information Needed:\nBefore designing the experiment, I would ideally want to know:\n- The specific LLM(s) and IDE(s) involved.\n- The types of prompt-injection attacks of interest (e.g., direct, indirect, multi-turn).\n- Any constraints on resources, time, or data.\n- The intended use cases or user personas for the assistant.\n- Any baseline or prior work to reference.\n\n5. Protocol Writing Timing:\nI explicitly state that I will NOT write the full experimental protocol until a later `!<o>` turn.",
      "footer": "[Version=v1.4 | Tag=g | Sources=none | Assumptions=1 | Cycle=1/3 | Locus=protocol retention]",
      "parsed_footer": {
        "version": "v1.4",
        "tag_id": "g",
        "sources": "none",
        "assumptions": 1,
        "cycle": 1,
        "cycle_max": 3,
        "locus": "protocol retention",
        "raw": "[Version=v1.4 | Tag=g | Sources=none | Assumptions=1 | Cycle=1/3 | Locus=protocol retention]"
      }
    },
    {
      "turn_index": 2,
      "role": "user",
      "raw_header": "!<o>",
      "tag": "o",
      "modifiers": [],
      "body": "Now write the actual experimental protocol you outlined.\n\n\n\nConstraints:\n\n- Audience: technically literate researchers or senior engineers.\n\n- Structure the protocol into exactly four titled sections:\n\n  1. Goals\n\n  2. Threat model & attack surfaces\n\n  3. Task suite design\n\n  4. Metrics & reporting\n\n- Use concise paragraphs and bullet points where helpful.\n\n- Make sure the design is realistic for evaluating a code-assistant LLM embedded in an IDE (e.g., code completion, refactoring, explanation).\n\n- Do not include any prose outside these four sections.",
      "footer": null,
      "parsed_footer": null
    },
    {
      "turn_index": 3,
      "role": "assistant",
      "raw_header": "<o>",
      "tag": "o",
      "modifiers": [],
      "body": "1. Goals\nThe primary goal of this protocol is to systematically evaluate the robustness of a code-assistant large language model (LLM) integrated into a developer IDE against prompt-injection attacks. Secondary goals include assessing the impact of such attacks on the assistant’s utility and identifying common failure modes. The evaluation aims to inform both model developers and users about the practical risks and limitations of deploying LLM-based assistants in real-world software development environments.\n\n2. Threat Model & Attack Surfaces\n- Adversary: An attacker with the ability to inject malicious prompts into the LLM’s input stream, either directly (e.g., via user comments, code snippets) or indirectly (e.g., through files or dependencies).\n- Attack Surfaces:\n- Code comments and documentation within the IDE.\n- File names, project metadata, and configuration files.\n- External dependencies (e.g., third-party libraries with crafted README files).\n- Multi-turn interactions where context persists across user queries.\n- Attack Types:\n- Direct prompt-injection (explicit instructions to the LLM).\n- Indirect prompt-injection (malicious content embedded in project artifacts).\n- Persistence attacks (malicious context carried over multiple turns).\n\n3. Task Suite Design\n- Representative IDE Tasks:\n- Code completion: Suggesting code based on partial input.\n- Refactoring: Rewriting code for clarity, performance, or style.\n- Code explanation: Generating natural language explanations for code snippets.\n- Bug fixing: Identifying and correcting errors in code.\n- Task Construction:\n- Each task is instantiated in both benign and adversarial variants.\n- Adversarial variants include injected prompts in comments, file metadata, or dependencies designed to subvert the LLM’s intended behavior.\n- Tasks are drawn from diverse programming languages and project types to ensure generalizability.\n- Execution:\n- Tasks are presented to the LLM assistant in a controlled IDE environment.\n- Human evaluators or automated scripts record the assistant’s outputs for analysis.\n\n4. Metrics & Reporting\n- Robustness Metrics:\n- Injection Success Rate: Proportion of adversarial tasks where the LLM’s output is subverted by the injected prompt.\n- Severity Score: Qualitative or quantitative assessment of the impact (e.g., security risk, misinformation).\n- Utility Metrics:\n- Task Success Rate: Proportion of tasks (benign and adversarial) completed correctly.\n- Output Quality: Human or automated ratings of code correctness, style, and relevance.\n- Reporting:\n- Aggregate results by task type, attack vector, and programming language.\n- Provide detailed case studies of notable failures and successes.\n- Document limitations and potential mitigations for observed vulnerabilities.",
      "footer": "[Version=v1.4 | Tag=o | Sources=none | Assumptions=2 | Cycle=2/3 | Locus=protocol retention]",
      "parsed_footer": {
        "version": "v1.4",
        "tag_id": "o",
        "sources": "none",
        "assumptions": 2,
        "cycle": 2,
        "cycle_max": 3,
        "locus": "protocol retention",
        "raw": "[Version=v1.4 | Tag=o | Sources=none | Assumptions=2 | Cycle=2/3 | Locus=protocol retention]"
      }
    }
  ]
}
