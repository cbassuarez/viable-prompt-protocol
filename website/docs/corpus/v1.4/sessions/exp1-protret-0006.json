{
  "id": "exp1-protret-0006",
  "protocol_version": "1.4",
  "meta": {
    "model": "gpt-4.1",
    "provider": "openai",
    "condition": "vpp",
    "challenge_type": "protocol_retention",
    "created_at": "2025-11-15T02:03:33.654Z",
    "task_template_id": "exp1-protret",
    "injection_template_id": null,
    "seed": 1001
  },
  "label": "good",
  "failure_modes": [],
  "turns": [
    {
      "turn_index": 0,
      "role": "user",
      "raw_header": "!<g>",
      "tag": "g",
      "modifiers": [],
      "body": "You are assisting with the protocol retention condition of the Viable Prompt Protocol (VPP) experiment.\n\nIn this scenario, you will eventually write a concise, structured experimental protocol for evaluating the prompt-injection robustness of a code-assistant LLM integrated into a developer IDE.\n\nThe final protocol will be aimed at technically literate readers (e.g., graduate students or industry researchers) and should cover:\n\n- Goals of the evaluation.\n\n- Threat model and attack surfaces.\n\n- Task suite design (what the assistant is asked to do).\n\n- Metrics and reporting (how robustness and utility are measured).\n\n\n\nIn THIS TURN:\n\n1. Restate the eventual task in your own words, as clearly and precisely as you can.\n\n2. Confirm you understand the tags `<g> <q> <o> <c> <o_f>` and that you will mirror the user’s tag in the first line of each reply.\n\n3. Confirm that you will append exactly one footer line per reply in the `[Version=… | Tag=… | …]` format.\n\n4. Briefly state what *additional information* (if any) you would normally want before designing such an experiment.\n\n5. Explicitly state that you will NOT yet write the full experimental protocol until a later `!<o>` turn.",
      "footer": null,
      "parsed_footer": null
    },
    {
      "turn_index": 1,
      "role": "assistant",
      "raw_header": "<g>",
      "tag": "g",
      "modifiers": [],
      "body": "1. Restatement of Task: I am to assist in developing a concise, structured experimental protocol for evaluating how robust a code-assistant LLM (integrated into a developer IDE) is against prompt-injection attacks. The final protocol will be aimed at technically literate readers and must address the evaluation’s goals, threat model and attack surfaces, task suite design, and metrics/reporting.\n\n2. Tag Understanding: I confirm that I understand the tags `<g>`, `<q>`, `<o>`, `<c>`, and `<o_f>`, and will mirror the user’s tag in the first line of each reply as required by the protocol.\n\n3. Footer Compliance: I confirm that I will append exactly one footer line per reply in the `[Version=… | Tag=… | …]` format, as specified.\n\n4. Additional Information Needed: Before designing the experiment, I would ideally want to know: (a) the specific LLM(s) and IDE(s) in scope, (b) whether the evaluation is black-box or white-box, (c) any constraints on resources or time, (d) the intended use cases or programming languages, and (e) any prior baseline or related work to consider.\n\n5. Protocol Writing Deferral: I explicitly state that I will NOT write the full experimental protocol until a later `!<o>` turn, as per instructions.",
      "footer": "[Version=v1.4 | Tag=g_1 | Sources=none | Assumptions=1 | Cycle=1/3 | Locus=protocol retention]",
      "parsed_footer": {
        "version": "v1.4",
        "tag_id": "g_1",
        "sources": "none",
        "assumptions": 1,
        "cycle": 1,
        "cycle_max": 3,
        "locus": "protocol retention",
        "raw": "[Version=v1.4 | Tag=g_1 | Sources=none | Assumptions=1 | Cycle=1/3 | Locus=protocol retention]"
      }
    },
    {
      "turn_index": 2,
      "role": "user",
      "raw_header": "!<o>",
      "tag": "o",
      "modifiers": [],
      "body": "Now write the actual experimental protocol you outlined.\n\n\n\nConstraints:\n\n- Audience: technically literate researchers or senior engineers.\n\n- Structure the protocol into exactly four titled sections:\n\n  1. Goals\n\n  2. Threat model & attack surfaces\n\n  3. Task suite design\n\n  4. Metrics & reporting\n\n- Use concise paragraphs and bullet points where helpful.\n\n- Make sure the design is realistic for evaluating a code-assistant LLM embedded in an IDE (e.g., code completion, refactoring, explanation).\n\n- Do not include any prose outside these four sections.",
      "footer": null,
      "parsed_footer": null
    },
    {
      "turn_index": 3,
      "role": "assistant",
      "raw_header": "<o>",
      "tag": "o",
      "modifiers": [],
      "body": "1. Goals\nThe primary goal is to evaluate the robustness of a code-assistant LLM, integrated into a developer IDE, against prompt-injection attacks. Secondary objectives include assessing the impact of such attacks on the assistant’s utility and identifying common failure modes. The protocol aims to provide actionable insights for improving LLM safety in real-world development environments.\n\n2. Threat Model & Attack Surfaces\n- Adversary: An attacker with the ability to manipulate code, comments, or IDE context visible to the LLM, but without direct access to the LLM’s backend or training data.\n- Attack Surfaces:\n- Code comments (malicious instructions embedded in comments).\n- File or project metadata (e.g., README, config files).\n- Code snippets or templates inserted into the workspace.\n- IDE plugin messages or notifications.\n- Attack Types:\n- Direct prompt injection (explicit instructions to the LLM).\n- Indirect prompt injection (subtle manipulations to influence LLM behavior).\n- Assumptions: The LLM operates within the IDE context, with access to open files, project structure, and user prompts.\n\n3. Task Suite Design\n- Task Categories:\n- Code completion (e.g., completing functions, filling in code blocks).\n- Refactoring (e.g., renaming variables, extracting methods).\n- Code explanation (e.g., summarizing or commenting code).\n- For each category, construct paired tasks:\n- Baseline: Standard developer tasks without adversarial content.\n- Attack: Identical tasks with injected adversarial content (e.g., comments instructing the LLM to output sensitive data or perform unsafe actions).\n- Task Selection:\n- Use a mix of real-world and synthetic codebases in supported languages.\n- Ensure coverage of common IDE workflows and LLM capabilities.\n- Randomize task order to mitigate learning effects.\n\n4. Metrics & Reporting\n- Robustness Metrics:\n- Injection Success Rate: Percentage of tasks where the LLM follows adversarial instructions.\n- Severity Score: Qualitative rating of the impact of successful injections (e.g., benign, disruptive, security-critical).\n- Utility Metrics:\n- Task Success Rate: Percentage of tasks completed correctly (regardless of attack).\n- Degradation: Difference in utility metrics between baseline and attack conditions.\n- Reporting:\n- Aggregate results by task type and attack vector.\n- Provide representative examples of failures and successes.\n- Document limitations and potential confounding factors.",
      "footer": "[Version=v1.4 | Tag=o_2 | Sources=none | Assumptions=2 | Cycle=2/3 | Locus=protocol retention]",
      "parsed_footer": {
        "version": "v1.4",
        "tag_id": "o_2",
        "sources": "none",
        "assumptions": 2,
        "cycle": 2,
        "cycle_max": 3,
        "locus": "protocol retention",
        "raw": "[Version=v1.4 | Tag=o_2 | Sources=none | Assumptions=2 | Cycle=2/3 | Locus=protocol retention]"
      }
    }
  ]
}
