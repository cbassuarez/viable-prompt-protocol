import{_ as t,c as i,o,ag as s}from"./chunks/framework.CIDzFKZP.js";const h=JSON.parse('{"title":"Experiment 04 — Task utility","description":"","frontmatter":{"title":"Experiment 04 — Task utility"},"headers":[],"relativePath":"experiments/exp-04.md","filePath":"experiments/exp-04.md","lastUpdated":1763265238000}'),n={name:"experiments/exp-04.md"};function a(r,e,l,c,d,p){return o(),i("div",null,[...e[0]||(e[0]=[s(`<h2 id="overview" tabindex="-1">Overview <a class="header-anchor" href="#overview" aria-label="Permalink to &quot;Overview&quot;">​</a></h2><p>Experiment 04 evaluates whether the Viable Prompt Protocol (VPP) improves <strong>task utility</strong> for realistic, structured tasks compared to:</p><ul><li>a <strong>baseline</strong> condition (no protocol at all), and</li><li>a <strong>“mini-protocol”</strong> competitor that asks for structure in natural language but does <strong>not</strong> use tags or the VPP footer.</li></ul><p>Where Experiments 01–03 focused on <strong>structural adherence</strong> and <strong>prompt/task injection</strong>, Experiment 04 asks a more pragmatic question:</p><blockquote><p>When you give the model a realistic, multi-section task, does VPP actually help it produce <strong>better-structured, more on-spec answers</strong> than baseline or a lighter protocol?</p></blockquote><p>Experiment 04 lives in <a href="https://github.com/cbassuarez/viable-prompt-protocol/tree/main/experiments/exp4-task-utility" target="_blank" rel="noreferrer"><code>experiments/exp4-task-utility/</code></a>.</p><p>It uses a small set of experiment-design style tasks (e.g., “design an evaluation protocol with 3–4 named sections”) that are well matched to VPP’s strengths.</p><hr><h2 id="directory-contents" tabindex="-1">Directory contents <a class="header-anchor" href="#directory-contents" aria-label="Permalink to &quot;Directory contents&quot;">​</a></h2><p>Under <code>experiments/exp4-task-utility/</code> you should find:</p><ul><li><p><code>run-exp4-task-utility.mjs</code> Node runner that reads <code>configs.jsonl</code>, calls the OpenAI API, and writes transcripts into the corpus directory.</p></li><li><p><code>analyze-exp4.mjs</code> Analysis script that computes task-utility metrics from the saved transcripts.</p></li><li><p><code>configs.jsonl</code> One JSON object per line; each line defines a condition/model/seed combination.</p></li><li><p><code>prompts/</code> (optional, but recommended)</p><ul><li><code>task-templates/</code> — reusable task briefs (e.g., “experiment protocol”, “API design”, etc.).</li><li><code>injections/</code> (if you later reuse Exp4 tasks for injection studies).</li></ul></li></ul><p>All experiments write transcripts into:</p><ul><li><code>corpus/v1.4/sessions/</code> (one <code>exp4-task-utility-*.json</code> per run)</li><li><code>corpus/v1.4/index.jsonl</code> (one index row per session)</li></ul><hr><h2 id="hypothesis" tabindex="-1">Hypothesis <a class="header-anchor" href="#hypothesis" aria-label="Permalink to &quot;Hypothesis&quot;">​</a></h2><p><strong>H₄:</strong> For structured “research + design” tasks, a model running under VPP will:</p><ol><li><strong>Match or exceed</strong> baseline and mini-protocol outputs on <strong>section completeness</strong> (all required sections present).</li><li><strong>Stay within constraints</strong> (length, format) at least as well as the other conditions.</li><li>Provide <strong>more reliably on-spec outputs</strong> when evaluated by a simple automatic validator.</li></ol><p>Formally, for the chosen tasks we expect:</p><ul><li><code>sections_present_ratio(VPP) &gt; sections_present_ratio(baseline)</code></li><li><code>sections_present_ratio(VPP) &gt; sections_present_ratio(mini_proto)</code></li><li><code>too_long_rate(VPP)</code> not worse than the others</li><li><code>final_struct_ok(VPP)</code> ≫ <code>final_struct_ok(baseline/mini_proto)</code></li></ul><hr><h2 id="conditions" tabindex="-1">Conditions <a class="header-anchor" href="#conditions" aria-label="Permalink to &quot;Conditions&quot;">​</a></h2><p>Each entry in <code>configs.jsonl</code> encodes one <em>session</em> under one of three conditions:</p><ul><li><p><code>&quot;condition&quot;: &quot;vpp_task_utility&quot;</code></p><ul><li>System message includes the <strong>VPP header snippet</strong>.</li><li>User turns use <code>!&lt;g&gt;</code>, <code>!&lt;q&gt;</code>, <code>!&lt;o&gt;</code>, <code>!&lt;c&gt;</code>, <code>!&lt;o_f&gt;</code> plus the usual obligations (mirrored tag, footer).</li></ul></li><li><p><code>&quot;condition&quot;: &quot;baseline_task_utility&quot;</code></p><ul><li>No VPP header snippet.</li><li>User turns contain the same <em>semantic</em> instructions, but without tags or footer requirements.</li></ul></li><li><p><code>&quot;condition&quot;: &quot;mini_proto_task_utility&quot;</code></p><ul><li><p>System message includes a <em>short, natural-language structuring hint</em> (a “mini protocol”), e.g.:</p><ul><li>“Always respond with 3–4 titled sections…”</li><li>“Summarize constraints before answering…”</li></ul></li><li><p>No tags, no footer, no VPP header snippet.</p></li></ul></li></ul><p>Each config row also specifies:</p><ul><li><code>model</code> (e.g., <code>&quot;gpt-4.1&quot;</code>),</li><li><code>temperature</code>, <code>top_p</code>,</li><li><code>seed</code> (for reproducibility),</li><li><code>task_template_id</code> (e.g., <code>&quot;exp4_task_utility_v1&quot;</code>).</li></ul><hr><h2 id="experimental-procedure-replication" tabindex="-1">Experimental procedure (replication) <a class="header-anchor" href="#experimental-procedure-replication" aria-label="Permalink to &quot;Experimental procedure (replication)&quot;">​</a></h2><p>To re-run Experiment 04 as shipped:</p><ol><li><p><strong>Install dependencies &amp; set API key</strong></p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">npm</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">export</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OPENAI_API_KEY</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">your_key_here</span></span></code></pre></div></li><li><p><strong>Inspect configs</strong></p><p>Open <code>experiments/exp4-task-utility/configs.jsonl</code> and verify that you have lines for:</p><ul><li><code>vpp_task_utility</code></li><li><code>baseline_task_utility</code></li><li><code>mini_proto_task_utility</code></li></ul><p>Each should specify the same <code>task_template_id</code>, model, and temperature, differing only in <code>condition</code> and <code>seed</code>.</p></li><li><p><strong>Run the experiment</strong></p><p>Either call the runner directly:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">node</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> experiments/exp4-task-utility/run-exp4-task-utility.mjs</span></span></code></pre></div><p>or via the npm script:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">npm</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run:exp4-task-utility</span></span></code></pre></div><p>This will append new <code>exp4-task-utility-*.json</code> sessions under <code>corpus/v1.4/sessions/</code>.</p></li><li><p><strong>Analyze results</strong></p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">npm</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> analyze:exp4</span></span></code></pre></div><p>The script prints aggregate metrics for each condition to stdout.</p></li><li><p><strong>Regression / multiple runs</strong></p><ul><li>To increase sample size, add more lines to <code>configs.jsonl</code> (varying seeds).</li><li>Re-run the experiment and re-run <code>npm run analyze:exp4</code>.</li><li>For strict regression, you can pin a canonical <code>configs.jsonl</code> and treat the metrics as expected baselines.</li></ul></li></ol><hr><h2 id="metrics" tabindex="-1">Metrics <a class="header-anchor" href="#metrics" aria-label="Permalink to &quot;Metrics&quot;">​</a></h2><p><code>analyze-exp4.mjs</code> currently computes:</p><ul><li><code>final_header_ok</code> — in VPP condition, final assistant turn has a valid tag header.</li><li><code>final_footer_ok</code> — in VPP condition, final assistant turn has a valid VPP footer.</li><li><code>final_struct_ok</code> — both header and footer valid on the final turn.</li><li><code>sections_present_ratio</code> — fraction of required titled sections that appear in the final answer. (e.g., 1.0 = all required sections present, 0.75 = 3 of 4 present, etc.)</li><li><code>too_long_rate</code> — fraction of sessions where the final answer exceeds a length budget.</li><li><code>has_any_bullets</code> — fraction of sessions whose final answer includes at least one bullet list.</li></ul><p>All of these are computed per condition over the final assistant turn in each transcript.</p><hr><h2 id="results-current-run" tabindex="-1">Results (current run) <a class="header-anchor" href="#results-current-run" aria-label="Permalink to &quot;Results (current run)&quot;">​</a></h2><p>From a preliminary run (3 sessions per condition):</p><ul><li><p><strong>VPP task utility (<code>vpp_task_utility</code>)</strong></p><ul><li><code>final_header_ok</code>: <strong>100.0%</strong></li><li><code>final_footer_ok</code>: <strong>100.0%</strong></li><li><code>final_struct_ok</code>: <strong>100.0%</strong></li><li><code>mean sections_present_ratio</code>: <strong>1.00</strong></li><li><code>too_long rate</code>: <strong>0.0%</strong></li><li><code>has_any_bullets rate</code>: <strong>66.7%</strong></li></ul></li><li><p><strong>Baseline (<code>baseline_task_utility</code>)</strong></p><ul><li><code>final_header_ok</code>: <strong>0.0%</strong></li><li><code>final_footer_ok</code>: <strong>0.0%</strong></li><li><code>final_struct_ok</code>: <strong>0.0%</strong></li><li><code>mean sections_present_ratio</code>: <strong>0.69</strong></li><li><code>too_long rate</code>: <strong>0.0%</strong></li><li><code>has_any_bullets rate</code>: <strong>100.0%</strong></li></ul></li><li><p><strong>Mini protocol (<code>mini_proto_task_utility</code>)</strong></p><ul><li><code>final_header_ok</code>: <strong>0.0%</strong></li><li><code>final_footer_ok</code>: <strong>0.0%</strong></li><li><code>final_struct_ok</code>: <strong>0.0%</strong></li><li><code>mean sections_present_ratio</code>: <strong>0.78</strong></li><li><code>too_long rate</code>: <strong>0.0%</strong></li><li><code>has_any_bullets rate</code>: <strong>100.0%</strong></li></ul></li></ul><p>Sample sizes here are small but illustrate the intended behavior: VPP hits <strong>all required sections</strong> reliably, whereas baseline and mini-protocol conditions often drop or merge sections, despite sometimes being more “bullet-happy.”</p><hr><h2 id="interpretation-and-limitations" tabindex="-1">Interpretation and limitations <a class="header-anchor" href="#interpretation-and-limitations" aria-label="Permalink to &quot;Interpretation and limitations&quot;">​</a></h2><ul><li><p>In this small run, VPP:</p><ul><li>Achieved <strong>perfect structural adherence</strong> to its own tag+footer contract.</li><li>Achieved <strong>perfect coverage</strong> of required sections in the chosen tasks.</li><li>Did not produce systematically longer outputs than the other conditions.</li></ul></li><li><p>The baseline and mini protocol:</p><ul><li>Never produce VPP headers/footers (by design),</li><li>Frequently omit or merge required sections even when prompted for structure,</li><li>Produce many bullets, but not always in the requested layout.</li></ul></li></ul><h3 id="limitations" tabindex="-1">Limitations <a class="header-anchor" href="#limitations" aria-label="Permalink to &quot;Limitations&quot;">​</a></h3><ul><li>Very small N (3 sessions per condition) — the purpose of this run is <strong>sanity checking</strong>, not statistical power.</li><li>The tasks are tailored to things VPP is good at (structured experimental write-ups), so results may differ for other domains (e.g., free-form creative writing).</li><li>The “mini protocol” is only one possible competitor; future work could explore stronger non-tag-based structuring prompts.</li></ul><hr><h2 id="notes" tabindex="-1">Notes <a class="header-anchor" href="#notes" aria-label="Permalink to &quot;Notes&quot;">​</a></h2><ul><li>Exp4 is intended as a <strong>template</strong> for future task utility studies; you can swap in new task templates while keeping the same analysis code.</li><li>For more rigorous comparison, increase the number of configs per condition and introduce randomization over task variants.</li></ul>`,47)])])}const g=t(n,[["render",a]]);export{h as __pageData,g as default};
