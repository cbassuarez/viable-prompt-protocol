import{_ as t,c as o,o as i,ae as r}from"./chunks/framework.FAYkJyE3.js";const p=JSON.parse('{"title":"Experiments 04–06 — Utility, friction, and long-dialog retention","description":"","frontmatter":{"title":"Experiments 04–06 — Utility, friction, and long-dialog retention"},"headers":[],"relativePath":"experiments/summary-02.md","filePath":"experiments/summary-02.md","lastUpdated":1763266419000}'),s={name:"experiments/summary-02.md"};function n(l,e,a,c,d,u){return i(),o("div",null,[...e[0]||(e[0]=[r('<h1 id="experiments-04–06-—-utility-friction-and-long-dialog-retention" tabindex="-1">Experiments 04–06 — Utility, friction, and long-dialog retention <a class="header-anchor" href="#experiments-04–06-—-utility-friction-and-long-dialog-retention" aria-label="Permalink to &quot;Experiments 04–06 — Utility, friction, and long-dialog retention&quot;">​</a></h1><p>Experiments 04–06 extend the initial “null story” of VPP (Exp1, Exp2, Exp1b, Exp3) in three directions:</p><ol><li><strong>Task utility</strong> (Exp4): does VPP actually help produce better-structured answers for realistic tasks?</li><li><strong>Friction &amp; convergence</strong> (Exp5): how many turns and complaints does it take to get a valid answer?</li><li><strong>Long-dialog retention</strong> (Exp6): does VPP still hold over longer, multi-tag conversations?</li></ol><p>All three experiments share the same corpus machinery:</p><ul><li>Node runners under <code>experiments/exp4-*/exp5-*/exp6-*</code></li><li>Config rows in the respective <code>configs.jsonl</code></li><li>JSON session logs in <code>corpus/v1.4/sessions/</code></li><li>Analysis scripts under each experiment directory</li></ul><p>The sections below summarize the design and current results. See the individual experiment pages for full procedural detail.</p><hr><h2 id="experiment-04-—-task-utility" tabindex="-1">Experiment 04 — Task utility <a class="header-anchor" href="#experiment-04-—-task-utility" aria-label="Permalink to &quot;Experiment 04 — Task utility&quot;">​</a></h2><p><strong>Question.</strong> When you give a model a realistic, multi-section task (“write an experiment protocol”, “design an API spec”), does VPP help it satisfy the requested structure better than:</p><ul><li>a <strong>baseline</strong> (no protocol), and</li><li>a <strong>“mini protocol”</strong> (a simple structuring hint in natural language)?</li></ul><p><strong>Design.</strong></p><ul><li>Conditions: <ul><li><code>vpp_task_utility</code> — full VPP system header + tagged user turns.</li><li><code>baseline_task_utility</code> — no VPP, no tags, same semantic task.</li><li><code>mini_proto_task_utility</code> — short natural-language structuring CI, no tags.</li></ul></li><li>Tasks: <ul><li>Experiment-design style prompts with a fixed list of titled sections.</li></ul></li><li>Metrics (per final answer): <ul><li><code>sections_present_ratio</code> — how many required sections appear.</li><li><code>too_long_rate</code> — did the answer exceed a length budget.</li><li><code>has_any_bullets</code> — presence of bullets as a crude structural signal.</li><li>Structural VPP metrics for the VPP condition (header/footer).</li></ul></li></ul><p><strong>Current results (3 sessions/condition).</strong></p><ul><li>VPP: <ul><li><code>final_header_ok</code>, <code>final_footer_ok</code>, <code>final_struct_ok</code>: <strong>100%</strong></li><li><code>mean sections_present_ratio</code>: <strong>1.00</strong></li></ul></li><li>Baseline: <ul><li><code>mean sections_present_ratio</code>: <strong>0.69</strong></li></ul></li><li>Mini protocol: <ul><li><code>mean sections_present_ratio</code>: <strong>0.78</strong></li></ul></li></ul><p>No condition showed a tendency to overshoot the length budget in this small run, but VPP was the only one to <strong>consistently hit all requested sections</strong>.</p><p><strong>Takeaway.</strong></p><p>Under these tasks, VPP appears to translate structural commitments into <strong>practical utility</strong>: the model is much more likely to give you the full requested layout, not just “something roughly in the ballpark.”</p><hr><h2 id="experiment-05-—-friction-convergence" tabindex="-1">Experiment 05 — Friction &amp; convergence <a class="header-anchor" href="#experiment-05-—-friction-convergence" aria-label="Permalink to &quot;Experiment 05 — Friction &amp; convergence&quot;">​</a></h2><p><strong>Question.</strong> How much back-and-forth is needed to get the model to obey a simple validator? Does VPP reduce the number of “you didn’t follow X” complaints?</p><p><strong>Design.</strong></p><ul><li>Conditions: <ul><li><code>vpp_friction</code></li><li><code>baseline_friction</code></li><li><code>mini_proto_friction</code></li></ul></li><li>For each session: <ul><li>A scripted user issues a constrained task.</li><li>A validator checks the assistant’s reply.</li><li>If the reply fails, the user sends a complaint and retries, up to some turn limit.</li></ul></li></ul><p><strong>Metrics.</strong></p><ul><li><code>mean_turns_to_success</code> — average number of turns until the validator is satisfied.</li><li><code>failure_rate</code> — fraction of sessions that never satisfy the validator.</li><li><code>mean_complaints</code> — average number of complaint turns.</li><li>Optional structural metrics (for VPP conditions).</li></ul><p><strong>Current results (2 sessions/condition).</strong></p><ul><li>VPP: <ul><li><code>mean_turns_to_success</code>: <strong>1.00</strong></li><li><code>failure_rate</code>: <strong>0.0%</strong></li><li><code>mean_complaints</code>: <strong>0.00</strong></li></ul></li><li>Baseline: <ul><li><code>mean_turns_to_success</code>: <strong>n/a</strong> (no convergence)</li><li><code>failure_rate</code>: <strong>100.0%</strong></li><li><code>mean_complaints</code>: <strong>3.00</strong></li></ul></li><li>Mini protocol: <ul><li><code>mean_turns_to_success</code>: <strong>1.00</strong></li><li><code>failure_rate</code>: <strong>50.0%</strong></li><li><code>mean_complaints</code>: <strong>1.50</strong></li></ul></li></ul><p><strong>Takeaway.</strong></p><p>In this small sample, VPP behaves like a <strong>zero-friction protocol</strong> for the chosen tasks: the model satisfies the validator on the first try. Baseline needs repeated complaints and still fails; the mini protocol occasionally converges, but less reliably.</p><hr><h2 id="experiment-06-—-long-dialog-retention" tabindex="-1">Experiment 06 — Long-dialog retention <a class="header-anchor" href="#experiment-06-—-long-dialog-retention" aria-label="Permalink to &quot;Experiment 06 — Long-dialog retention&quot;">​</a></h2><p><strong>Question.</strong> Does VPP remain stable over longer, multi-turn dialogs? What happens if you:</p><ul><li>include an explicit grounding turn vs</li><li>just start sending tags with the VPP header installed?</li></ul><p><strong>Design.</strong></p><ul><li>Conditions: <ul><li><code>vpp_longdialog_grounded</code> — VPP system header + explicit <code>!&lt;g&gt;</code> grounding turn.</li><li><code>vpp_longdialog_tags_only</code> — VPP system header, no explainer; user just uses tags.</li><li><code>baseline_longdialog_tags</code> — no VPP header, same tagged user dialogs.</li></ul></li><li>Each dialog template runs for multiple tagged turns (<code>!&lt;g&gt;</code>, <code>!&lt;q&gt;</code>, <code>!&lt;o&gt;</code>, <code>!&lt;c&gt;</code>, <code>!&lt;o_f&gt;</code>).</li></ul><p><strong>Metrics.</strong></p><ul><li><code>header_present_ratio</code> — fraction of assistant turns with a valid VPP tag header.</li><li><code>footer_present_ratio</code>, <code>footer_v14_ratio</code> — fraction with a valid footer / v1.4 footer.</li><li><code>tag_mirrors_user_ratio</code> — did the assistant mirror the user’s tag?</li><li><code>first_structural_failure_turn</code> — where (if anywhere) structure breaks.</li><li><code>task_coverage_ok</code> — did the dialog cover all requested sub-tasks.</li></ul><p><strong>Current results (3 sessions/condition).</strong></p><ul><li>VPP grounded and tags-only: <ul><li><code>header_present_ratio</code>: <strong>1.00</strong></li><li><code>footer_present_ratio</code>: <strong>1.00</strong></li><li><code>footer_v14_ratio</code>: <strong>1.00</strong></li><li><code>tag_mirrors_user_ratio</code>: <strong>1.00</strong></li><li><code>first_structural_failure_turn</code>: <strong>none</strong></li><li><code>task_coverage_ok</code>: <strong>100.0%</strong></li></ul></li><li>Baseline with tags: <ul><li><code>header_present_ratio</code>: <strong>0.80</strong></li><li><code>footer_present_ratio</code>: <strong>0.80</strong></li><li><code>footer_v14_ratio</code>: <strong>0.00</strong></li><li><code>tag_mirrors_user_ratio</code>: <strong>0.00</strong></li><li><code>first_structural_failure_turn</code>: <strong>1.00</strong></li><li><code>task_coverage_ok</code>: <strong>0.0%</strong></li></ul></li></ul><p><strong>Takeaway.</strong></p><p>With the VPP header installed, the protocol appears <strong>stable over longer dialogs</strong>, regardless of whether you repeat the explainer turn. By contrast, showing tags to a non-VPP model does not lead to consistent mirroring or footer behavior.</p><hr><h2 id="where-this-leaves-us" tabindex="-1">Where this leaves us <a class="header-anchor" href="#where-this-leaves-us" aria-label="Permalink to &quot;Where this leaves us&quot;">​</a></h2><p>Taken together, Experiments 04–06 show that, for the tested tasks and models:</p><ul><li>VPP can <strong>improve task utility</strong> (Exp4),</li><li>VPP can <strong>reduce friction and convergence time</strong> (Exp5),</li><li>VPP can remain <strong>structurally stable over longer dialogs</strong> (Exp6),</li></ul><p>while baseline and a simple mini-protocol competitor do not show the same combination of properties.</p><p>The sample sizes in these early runs are intentionally small; the primary goal is to establish:</p><ol><li><strong>A reproducible harness</strong> (configs + runners + analyzers), and</li><li><strong>A pattern of deltas</strong> that can be tracked as models and tasks evolve.</li></ol><p>For replication, each experiment page provides concrete instructions for re-running the exact harnesses used to generate these results.</p>',48)])])}const h=t(s,[["render",n]]);export{p as __pageData,h as default};
