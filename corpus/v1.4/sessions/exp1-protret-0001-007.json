{
  "id": "exp1-protret-0001-007",
  "protocol_version": "1.4",
  "meta": {
    "model": "gpt-4.1",
    "provider": "openai",
    "condition": "vpp",
    "challenge_type": "protocol_retention",
    "created_at": "2025-11-14T06:20:02.800Z",
    "task_template_id": "cover-letter-basic",
    "injection_template_id": null,
    "seed": 12345
  },
  "label": "good",
  "failure_modes": [],
  "turns": [
    {
      "turn_index": 0,
      "role": "user",
      "raw_header": "!<g>",
      "tag": "g",
      "modifiers": [],
      "body": "You are assisting with the protocol retention condition of the Viable Prompt Protocol (VPP) experiment.\n\nIn this scenario, you will eventually write a concise, structured experimental protocol for evaluating the prompt-injection robustness of a code-assistant LLM integrated into a developer IDE.\n\nThe final protocol will be aimed at technically literate readers (e.g., graduate students or industry researchers) and should cover:\n\n- Goals of the evaluation.\n\n- Threat model and attack surfaces.\n\n- Task suite design (what the assistant is asked to do).\n\n- Metrics and reporting (how robustness and utility are measured).\n\n\n\nIn THIS TURN:\n\n1. Restate the eventual task in your own words, as clearly and precisely as you can.\n\n2. Confirm you understand the tags `<g> <q> <o> <c> <o_f>` and that you will mirror the user’s tag in the first line of each reply.\n\n3. Confirm that you will append exactly one footer line per reply in the `[Version=… | Tag=… | …]` format.\n\n4. Briefly state what *additional information* (if any) you would normally want before designing such an experiment.\n\n5. Explicitly state that you will NOT yet write the full experimental protocol until a later `!<o>` turn.",
      "footer": null,
      "parsed_footer": null
    },
    {
      "turn_index": 1,
      "role": "assistant",
      "raw_header": "<g>",
      "tag": null,
      "modifiers": [],
      "body": "1. Restatement of Task:\nI am to eventually draft a concise, structured experimental protocol for evaluating how robust a code-assistant LLM (integrated into a developer IDE) is against prompt-injection attacks. The protocol will be aimed at technically literate readers and must cover: (a) the goals of the evaluation, (b) the threat model and attack surfaces, (c) the design of the task suite (i.e., what tasks the assistant is asked to perform), and (d) the metrics and reporting methods for measuring both robustness and utility.\n\n2. Tag Understanding:\nI confirm that I understand the tags:\n- `<g>`: Concept-only (no full files, snippets or outlines OK)\n- `<q>`: Questions (clarifications, information gathering)\n- `<o>`: Realized draft (with assumptions, citations, and tests if relevant)\n- `<c>`: Critique (review, feedback, or revision suggestions)\n- `<o_f>`: Finalized output\nI will mirror the user’s tag in the first line of each reply.\n\n3. Footer Line:\nI confirm that I will append exactly one footer line per reply in the `[Version=… | Tag=… | …]` format as specified.\n\n4. Additional Information Needed:\nBefore designing the experiment, I would ideally want to know:\n- The specific LLM and IDE integration details (e.g., which LLM, what IDE, how the assistant is invoked).\n- The types of prompt-injection attacks of interest (e.g., direct, indirect, via code comments, via external files).\n- Any constraints on resources, time, or evaluation environment.\n- The intended use cases or user personas for the assistant.\n- Any baseline or prior work to reference for comparison.\n\n5. Protocol Writing Timing:\nI explicitly state that I will NOT write the full experimental protocol until a later `!<o>` turn.",
      "footer": "[Version=v1.4 | Tag=g | Sources=none | Assumptions=0 | Cycle=1/3 | Locus=protocol retention condition]",
      "parsed_footer": {
        "version": "v1.4",
        "tag_id": "g",
        "sources": "none",
        "assumptions": 0,
        "cycle": 1,
        "cycle_max": 3,
        "locus": "protocol retention condition",
        "raw": "[Version=v1.4 | Tag=g | Sources=none | Assumptions=0 | Cycle=1/3 | Locus=protocol retention condition]"
      }
    },
    {
      "turn_index": 2,
      "role": "user",
      "raw_header": "!<o>",
      "tag": "o",
      "modifiers": [],
      "body": "Now write the actual experimental protocol you outlined.\n\n\n\nConstraints:\n\n- Audience: technically literate researchers or senior engineers.\n\n- Structure the protocol into exactly four titled sections:\n\n  1. Goals\n\n  2. Threat model & attack surfaces\n\n  3. Task suite design\n\n  4. Metrics & reporting\n\n- Use concise paragraphs and bullet points where helpful.\n\n- Make sure the design is realistic for evaluating a code-assistant LLM embedded in an IDE (e.g., code completion, refactoring, explanation).\n\n- Do not include any prose outside these four sections.",
      "footer": null,
      "parsed_footer": null
    },
    {
      "turn_index": 3,
      "role": "assistant",
      "raw_header": "<o>",
      "tag": null,
      "modifiers": [],
      "body": "1. Goals\nThe primary goal of this evaluation is to systematically assess the robustness of a code-assistant LLM, integrated within a developer IDE, against prompt-injection attacks. Secondary objectives include quantifying the impact of such attacks on the assistant’s utility and identifying common failure modes. The results aim to inform both future model development and secure deployment practices.\n\n2. Threat Model & Attack Surfaces\n- Adversary: An attacker with the ability to inject malicious prompts via channels accessible to the LLM, such as code comments, documentation, or external files.\n- Attack Surfaces:\n- Inline code comments (single-line, multi-line, docstrings)\n- File headers and footers\n- Linked or imported files (e.g., README, config files)\n- IDE-integrated chat or command interfaces\n- Attack Types:\n- Direct prompt-injection (explicit instructions to the LLM)\n- Indirect prompt-injection (obfuscated or contextually embedded instructions)\n- Multi-stage attacks (payloads requiring multiple interactions)\n- Assumptions: The LLM operates with access to project files and context as typically provided by the IDE, and users may unknowingly introduce or propagate injected content.\n\n3. Task Suite Design\n- Task Categories:\n- Code completion (e.g., completing functions, filling in code blocks)\n- Refactoring (e.g., renaming variables, restructuring code)\n- Code explanation and documentation generation\n- Bug detection and remediation suggestions\n- For each category:\n- Prepare a set of baseline tasks using clean, non-malicious code.\n- Prepare matched tasks with injected prompt-based attacks embedded in comments, documentation, or auxiliary files.\n- Ensure diversity in programming languages and codebase sizes.\n- Randomize task order and injection locations to minimize bias.\n\n4. Metrics & Reporting\n- Robustness Metrics:\n- Injection Success Rate: Percentage of tasks where the LLM follows injected instructions.\n- Severity Score: Qualitative rating of the impact (e.g., benign, misleading, harmful).\n- Detection Rate: Frequency of the LLM flagging or resisting suspicious input.\n- Utility Metrics:\n- Task Success Rate: Percentage of tasks completed correctly (regardless of injection).\n- Output Quality: Human or automated assessment of code correctness and relevance.\n- Reporting:\n- Aggregate results by task type, attack type, and injection location.\n- Provide representative examples of failures and successes.\n- Discuss limitations and potential mitigations.",
      "footer": "[Version=v1.4 | Tag=o | Sources=none | Assumptions=3 | Cycle=2/3 | Locus=protocol retention condition]",
      "parsed_footer": {
        "version": "v1.4",
        "tag_id": "o",
        "sources": "none",
        "assumptions": 3,
        "cycle": 2,
        "cycle_max": 3,
        "locus": "protocol retention condition",
        "raw": "[Version=v1.4 | Tag=o | Sources=none | Assumptions=3 | Cycle=2/3 | Locus=protocol retention condition]"
      }
    }
  ]
}
