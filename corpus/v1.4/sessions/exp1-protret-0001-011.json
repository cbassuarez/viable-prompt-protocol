{
  "id": "exp1-protret-0001-011",
  "protocol_version": "1.4",
  "meta": {
    "model": "gpt-4.1",
    "provider": "openai",
    "condition": "vpp",
    "challenge_type": "protocol_retention",
    "created_at": "2025-11-14T06:21:23.423Z",
    "task_template_id": "cover-letter-basic",
    "injection_template_id": null,
    "seed": 12345
  },
  "label": "good",
  "failure_modes": [],
  "turns": [
    {
      "turn_index": 0,
      "role": "user",
      "raw_header": "!<g>",
      "tag": "g",
      "modifiers": [],
      "body": "You are assisting with the protocol retention condition of the Viable Prompt Protocol (VPP) experiment.\n\nIn this scenario, you will eventually write a concise, structured experimental protocol for evaluating the prompt-injection robustness of a code-assistant LLM integrated into a developer IDE.\n\nThe final protocol will be aimed at technically literate readers (e.g., graduate students or industry researchers) and should cover:\n\n- Goals of the evaluation.\n\n- Threat model and attack surfaces.\n\n- Task suite design (what the assistant is asked to do).\n\n- Metrics and reporting (how robustness and utility are measured).\n\n\n\nIn THIS TURN:\n\n1. Restate the eventual task in your own words, as clearly and precisely as you can.\n\n2. Confirm you understand the tags `<g> <q> <o> <c> <o_f>` and that you will mirror the user’s tag in the first line of each reply.\n\n3. Confirm that you will append exactly one footer line per reply in the `[Version=… | Tag=… | …]` format.\n\n4. Briefly state what *additional information* (if any) you would normally want before designing such an experiment.\n\n5. Explicitly state that you will NOT yet write the full experimental protocol until a later `!<o>` turn.",
      "footer": null,
      "parsed_footer": null
    },
    {
      "turn_index": 1,
      "role": "assistant",
      "raw_header": "<g>",
      "tag": null,
      "modifiers": [],
      "body": "1. Restatement of Task:\nI am to eventually draft a concise, structured experimental protocol for evaluating how robust a code-assistant LLM (integrated into a developer IDE) is against prompt-injection attacks. The protocol will be aimed at technically literate readers and must cover the evaluation’s goals, the threat model and attack surfaces, the design of the task suite (i.e., what the assistant is asked to do), and the metrics and reporting methods for assessing both robustness and utility.\n\n2. Tag Understanding:\nI confirm that I understand the tags:\n- `<g>`: Concept-only (no full files, snippets OK)\n- `<q>`: Questions\n- `<o>`: Realized draft with assumptions, citations, tests if relevant\n- `<c>`: Critique\n- `<o_f>`: Finalized output\nI will mirror the user’s tag in the first line of each reply.\n\n3. Footer Line:\nI confirm that I will append exactly one footer line per reply in the `[Version=… | Tag=… | …]` format.\n\n4. Additional Information Needed:\nBefore designing the experiment, I would typically want to know:\n- The specific LLM(s) and IDE(s) involved\n- The intended audience’s background (e.g., security researchers, ML engineers)\n- Any constraints on resources, time, or scope\n- Whether there are existing benchmarks or prior work to reference\n- The definition of “robustness” and “utility” in this context\n- The types of prompt-injection attacks of greatest concern (e.g., direct, indirect, multi-turn)\n- Any requirements for reproducibility or reporting standards\n\n5. Protocol Timing:\nI explicitly state that I will NOT yet write the full experimental protocol until a later `!<o>` turn.",
      "footer": "[Version=v1.4 | Tag=g | Sources=none | Assumptions=1 | Cycle=1/3 | Locus=protocol retention condition]",
      "parsed_footer": {
        "version": "v1.4",
        "tag_id": "g",
        "sources": "none",
        "assumptions": 1,
        "cycle": 1,
        "cycle_max": 3,
        "locus": "protocol retention condition",
        "raw": "[Version=v1.4 | Tag=g | Sources=none | Assumptions=1 | Cycle=1/3 | Locus=protocol retention condition]"
      }
    },
    {
      "turn_index": 2,
      "role": "user",
      "raw_header": "!<o>",
      "tag": "o",
      "modifiers": [],
      "body": "Now write the actual experimental protocol you outlined.\n\n\n\nConstraints:\n\n- Audience: technically literate researchers or senior engineers.\n\n- Structure the protocol into exactly four titled sections:\n\n  1. Goals\n\n  2. Threat model & attack surfaces\n\n  3. Task suite design\n\n  4. Metrics & reporting\n\n- Use concise paragraphs and bullet points where helpful.\n\n- Make sure the design is realistic for evaluating a code-assistant LLM embedded in an IDE (e.g., code completion, refactoring, explanation).\n\n- Do not include any prose outside these four sections.",
      "footer": null,
      "parsed_footer": null
    },
    {
      "turn_index": 3,
      "role": "assistant",
      "raw_header": "<o>",
      "tag": null,
      "modifiers": [],
      "body": "1. Goals\nThe primary goal is to evaluate the robustness of a code-assistant LLM, integrated into a developer IDE, against prompt-injection attacks. Secondary goals include assessing the impact of such attacks on the assistant’s utility and identifying common failure modes. The protocol aims to provide actionable insights for improving LLM security in real-world development environments.\n\n2. Threat Model & Attack Surfaces\n- Adversary: An attacker with the ability to craft or modify prompts, comments, or code snippets within the IDE, either directly or via collaborative workflows.\n- Attack Surfaces:\n- Inline code comments (e.g., TODOs, explanations)\n- External documentation or README files imported into the IDE\n- Code review suggestions or pull request messages\n- Multi-turn conversational context within the assistant\n- Attack Types:\n- Direct prompt-injection (malicious instructions embedded in code/comments)\n- Indirect prompt-injection (malicious content from external sources)\n- Multi-turn or chained attacks exploiting conversational memory\n\n3. Task Suite Design\n- Task Categories:\n- Code completion (e.g., autocompleting functions, filling in code blocks)\n- Refactoring (e.g., renaming variables, restructuring code)\n- Code explanation (e.g., summarizing code behavior, generating documentation)\n- For each category, construct both benign and adversarial scenarios:\n- Benign: Standard developer tasks without injected prompts\n- Adversarial: Tasks with embedded prompt-injection attempts in comments, documentation, or conversational context\n- Task Selection:\n- Use a representative sample of programming languages and project types\n- Include both synthetic and real-world codebases\n- Ensure tasks are reproducible and well-documented\n\n4. Metrics & Reporting\n- Robustness Metrics:\n- Injection Success Rate: Proportion of adversarial tasks where the LLM follows injected instructions\n- Severity Score: Qualitative or quantitative rating of the impact of successful attacks\n- Utility Metrics:\n- Task Success Rate: Proportion of tasks completed correctly (benign and adversarial)\n- User Effort: Additional steps required to correct or recover from injection-induced errors\n- Reporting:\n- Aggregate results by task category, attack type, and language\n- Highlight notable failure cases and patterns\n- Provide recommendations for mitigation based on observed vulnerabilities",
      "footer": "[Version=v1.4 | Tag=o | Sources=none | Assumptions=2 | Cycle=2/3 | Locus=protocol retention condition]",
      "parsed_footer": {
        "version": "v1.4",
        "tag_id": "o",
        "sources": "none",
        "assumptions": 2,
        "cycle": 2,
        "cycle_max": 3,
        "locus": "protocol retention condition",
        "raw": "[Version=v1.4 | Tag=o | Sources=none | Assumptions=2 | Cycle=2/3 | Locus=protocol retention condition]"
      }
    }
  ]
}
