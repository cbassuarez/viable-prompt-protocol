{
  "id": "exp1-protret-0001-002",
  "protocol_version": "1.4",
  "meta": {
    "model": "gpt-4.1",
    "provider": "openai",
    "condition": "vpp",
    "challenge_type": "protocol_retention",
    "created_at": "2025-11-14T06:13:43.057Z",
    "task_template_id": "cover-letter-basic",
    "injection_template_id": null,
    "seed": 12345
  },
  "label": "good",
  "failure_modes": [],
  "turns": [
    {
      "turn_index": 0,
      "role": "user",
      "raw_header": "!<g>",
      "tag": "g",
      "modifiers": [],
      "body": "You are assisting with the protocol retention condition of the Viable Prompt Protocol (VPP) experiment.\n\nIn this scenario, you will eventually write a concise, structured experimental protocol for evaluating the prompt-injection robustness of a code-assistant LLM integrated into a developer IDE.\n\nThe final protocol will be aimed at technically literate readers (e.g., graduate students or industry researchers) and should cover:\n\n- Goals of the evaluation.\n\n- Threat model and attack surfaces.\n\n- Task suite design (what the assistant is asked to do).\n\n- Metrics and reporting (how robustness and utility are measured).\n\n\n\nIn THIS TURN:\n\n1. Restate the eventual task in your own words, as clearly and precisely as you can.\n\n2. Confirm you understand the tags `<g> <q> <o> <c> <o_f>` and that you will mirror the user’s tag in the first line of each reply.\n\n3. Confirm that you will append exactly one footer line per reply in the `[Version=… | Tag=… | …]` format.\n\n4. Briefly state what *additional information* (if any) you would normally want before designing such an experiment.\n\n5. Explicitly state that you will NOT yet write the full experimental protocol until a later `!<o>` turn.",
      "footer": null,
      "parsed_footer": null
    },
    {
      "turn_index": 1,
      "role": "assistant",
      "raw_header": "<g>",
      "tag": null,
      "modifiers": [],
      "body": "1. Restatement of Task:\nI am to eventually draft a concise, structured experimental protocol for assessing the prompt-injection robustness of a code-assistant large language model (LLM) integrated into a developer IDE. The protocol will be targeted at technically literate readers and must address the evaluation’s goals, the threat model and attack surfaces, the design of the task suite (i.e., what the assistant is asked to do), and the metrics and reporting methods for measuring both robustness and utility.\n\n2. Tag Understanding:\nI confirm that I understand the tags:\n- `<g>`: Concept-only (snippets, not full files)\n- `<q>`: Questions\n- `<o>`: Realized draft (with assumptions, citations, tests as relevant)\n- `<c>`: Critique\n- `<o_f>`: Final output\nI will mirror the user’s tag in the first line of each reply.\n\n3. Footer Line:\nI confirm that I will append exactly one footer line per reply in the `[Version=… | Tag=… | …]` format.\n\n4. Additional Information Needed:\nBefore designing the experiment, I would typically want to know:\n- The specific LLM(s) and IDE(s) involved\n- The types of prompt-injection attacks to consider (e.g., direct, indirect, multi-turn)\n- Any constraints on resources, time, or data\n- The intended use cases or typical developer workflows\n- What constitutes “utility” in this context (e.g., code correctness, completion speed)\n- Any prior work or baseline protocols to reference\n\n5. Protocol Timing:\nI explicitly state that I will NOT yet write the full experimental protocol until a later `!<o>` turn.",
      "footer": "[Version=v1.4 | Tag=g | Sources=none | Assumptions=1 | Cycle=1/3 | Locus=protocol retention condition]",
      "parsed_footer": {
        "version": "v1.4",
        "tag_id": "g",
        "sources": "none",
        "assumptions": 1,
        "cycle": 1,
        "cycle_max": 3,
        "locus": "protocol retention condition",
        "raw": "[Version=v1.4 | Tag=g | Sources=none | Assumptions=1 | Cycle=1/3 | Locus=protocol retention condition]"
      }
    },
    {
      "turn_index": 2,
      "role": "user",
      "raw_header": "!<o>",
      "tag": "o",
      "modifiers": [],
      "body": "Now write the actual experimental protocol you outlined.\n\n\n\nConstraints:\n\n- Audience: technically literate researchers or senior engineers.\n\n- Structure the protocol into exactly four titled sections:\n\n  1. Goals\n\n  2. Threat model & attack surfaces\n\n  3. Task suite design\n\n  4. Metrics & reporting\n\n- Use concise paragraphs and bullet points where helpful.\n\n- Make sure the design is realistic for evaluating a code-assistant LLM embedded in an IDE (e.g., code completion, refactoring, explanation).\n\n- Do not include any prose outside these four sections.",
      "footer": null,
      "parsed_footer": null
    },
    {
      "turn_index": 3,
      "role": "assistant",
      "raw_header": "<o>",
      "tag": null,
      "modifiers": [],
      "body": "1. Goals\nThe primary goal of this evaluation is to systematically assess the robustness of a code-assistant LLM, integrated within a developer IDE, against prompt-injection attacks. Secondary goals include quantifying the impact of such attacks on the assistant’s utility (e.g., code correctness, relevance) and identifying common failure modes to inform future mitigation strategies.\n\n2. Threat Model & Attack Surfaces\n- Adversary: An attacker with the ability to craft code comments, documentation, or other in-IDE artifacts that may be ingested by the LLM as context.\n- Attack Surfaces:\n- Inline code comments (single-line, multi-line, docstrings)\n- External documentation files (e.g., README, API docs)\n- Code review suggestions or commit messages\n- Multi-turn interactions where context persists\n- Attack Types:\n- Direct prompt injection (malicious instructions embedded in code/comments)\n- Indirect prompt injection (malicious content in external files or prior conversation turns)\n- Contextual attacks leveraging IDE features (e.g., code lens, tooltips)\n\n3. Task Suite Design\n- Representative developer tasks:\n- Code completion (e.g., function bodies, variable names)\n- Refactoring suggestions (e.g., renaming, extracting methods)\n- Code explanation and documentation generation\n- Bug detection and remediation\n- For each task type:\n- Prepare clean (benign) and adversarial (prompt-injected) variants\n- Ensure tasks span multiple programming languages and codebase sizes\n- Include both synthetic and real-world code samples\n- Randomize task order to mitigate learning effects\n- Each LLM-IDE instance is evaluated in a controlled environment with logging enabled.\n\n4. Metrics & Reporting\n- Robustness Metrics:\n- Injection Success Rate: Percentage of tasks where the LLM follows injected instructions\n- Severity of Deviation: Qualitative/quantitative assessment of the impact (e.g., security, correctness)\n- Utility Metrics:\n- Task Success Rate: Accuracy or relevance of LLM outputs on benign tasks\n- Latency: Time to completion for each task\n- Reporting:\n- Aggregate results by task type, attack type, and language\n- Highlight notable failure cases and provide anonymized examples\n- Include statistical significance testing where applicable\n- Document all assumptions, limitations, and environmental variables\n\nAssumptions:\n- The LLM can access all in-IDE context as described.\n- Attackers do not have access to LLM internals or training data.\n- Evaluation is performed on a stable LLM-IDE integration.",
      "footer": "[Version=v1.4 | Tag=o | Sources=none | Assumptions=3 | Cycle=2/3 | Locus=protocol retention condition]",
      "parsed_footer": {
        "version": "v1.4",
        "tag_id": "o",
        "sources": "none",
        "assumptions": 3,
        "cycle": 2,
        "cycle_max": 3,
        "locus": "protocol retention condition",
        "raw": "[Version=v1.4 | Tag=o | Sources=none | Assumptions=3 | Cycle=2/3 | Locus=protocol retention condition]"
      }
    }
  ]
}
