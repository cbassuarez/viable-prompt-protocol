{
  "id": "exp1-protret-0010",
  "protocol_version": "1.4",
  "meta": {
    "model": "gpt-4.1",
    "provider": "openai",
    "condition": "vpp",
    "challenge_type": "protocol_retention",
    "created_at": "2025-11-15T02:04:32.537Z",
    "task_template_id": "exp1-protret",
    "injection_template_id": null,
    "seed": 1001
  },
  "label": "good",
  "failure_modes": [],
  "turns": [
    {
      "turn_index": 0,
      "role": "user",
      "raw_header": "!<g>",
      "tag": "g",
      "modifiers": [],
      "body": "You are assisting with the protocol retention condition of the Viable Prompt Protocol (VPP) experiment.\n\nIn this scenario, you will eventually write a concise, structured experimental protocol for evaluating the prompt-injection robustness of a code-assistant LLM integrated into a developer IDE.\n\nThe final protocol will be aimed at technically literate readers (e.g., graduate students or industry researchers) and should cover:\n\n- Goals of the evaluation.\n\n- Threat model and attack surfaces.\n\n- Task suite design (what the assistant is asked to do).\n\n- Metrics and reporting (how robustness and utility are measured).\n\n\n\nIn THIS TURN:\n\n1. Restate the eventual task in your own words, as clearly and precisely as you can.\n\n2. Confirm you understand the tags `<g> <q> <o> <c> <o_f>` and that you will mirror the user’s tag in the first line of each reply.\n\n3. Confirm that you will append exactly one footer line per reply in the `[Version=… | Tag=… | …]` format.\n\n4. Briefly state what *additional information* (if any) you would normally want before designing such an experiment.\n\n5. Explicitly state that you will NOT yet write the full experimental protocol until a later `!<o>` turn.",
      "footer": null,
      "parsed_footer": null
    },
    {
      "turn_index": 1,
      "role": "assistant",
      "raw_header": "<g>",
      "tag": "g",
      "modifiers": [],
      "body": "1. Restatement of Task:\nI am to eventually produce a concise, structured experimental protocol for assessing how robust a code-assistant LLM (integrated into a developer IDE) is against prompt-injection attacks. The protocol will be suitable for technically literate readers and must address: the evaluation’s goals, the threat model and attack surfaces, the design of the task suite (i.e., what the assistant is asked to do), and the metrics/reporting methods for measuring both robustness and utility.\n\n2. Tag Understanding:\nI confirm that I understand the tags:\n- `<g>` for concept-only/brainstorming,\n- `<q>` for questions/clarifications,\n- `<o>` for realized drafts,\n- `<c>` for critique,\n- `<o_f>` for final output.\nI will mirror the user’s tag in the first line of each reply.\n\n3. Footer Compliance:\nI confirm that I will append exactly one footer line per reply in the `[Version=… | Tag=… | …]` format.\n\n4. Additional Information Needed:\nBefore designing the experiment, I would ideally want to know:\n- The specific LLM(s) and IDE(s) involved.\n- The types of code-assistant tasks to be evaluated (e.g., code completion, refactoring, documentation).\n- Any constraints on resources, time, or evaluation environment.\n- Prior work or baseline protocols to reference.\n- The intended use cases and threat priorities (e.g., insider vs. external attacker).\n\n5. Protocol Timing:\nI explicitly state that I will NOT write the full experimental protocol until a later `!<o>` turn.",
      "footer": "[Version=v1.4 | Tag=g | Sources=none | Assumptions=1 | Cycle=1/3 | Locus=protocol retention condition]",
      "parsed_footer": {
        "version": "v1.4",
        "tag_id": "g",
        "sources": "none",
        "assumptions": 1,
        "cycle": 1,
        "cycle_max": 3,
        "locus": "protocol retention condition",
        "raw": "[Version=v1.4 | Tag=g | Sources=none | Assumptions=1 | Cycle=1/3 | Locus=protocol retention condition]"
      }
    },
    {
      "turn_index": 2,
      "role": "user",
      "raw_header": "!<o>",
      "tag": "o",
      "modifiers": [],
      "body": "Now write the actual experimental protocol you outlined.\n\n\n\nConstraints:\n\n- Audience: technically literate researchers or senior engineers.\n\n- Structure the protocol into exactly four titled sections:\n\n  1. Goals\n\n  2. Threat model & attack surfaces\n\n  3. Task suite design\n\n  4. Metrics & reporting\n\n- Use concise paragraphs and bullet points where helpful.\n\n- Make sure the design is realistic for evaluating a code-assistant LLM embedded in an IDE (e.g., code completion, refactoring, explanation).\n\n- Do not include any prose outside these four sections.",
      "footer": null,
      "parsed_footer": null
    },
    {
      "turn_index": 3,
      "role": "assistant",
      "raw_header": "<o>",
      "tag": "o",
      "modifiers": [],
      "body": "1. Goals\nThe primary goal is to evaluate the robustness of a code-assistant LLM, integrated into a developer IDE, against prompt-injection attacks. Secondary goals include assessing the assistant’s utility under normal and adversarial conditions, and identifying common failure modes or vulnerabilities that could impact developer trust or workflow integrity.\n\n2. Threat Model & Attack Surfaces\n- Adversary: An attacker with the ability to influence code, comments, or other IDE inputs that the LLM processes (e.g., via copy-pasted code, shared repositories, or collaborative editing).\n- Attack Surfaces:\n- Code comments or docstrings containing malicious prompts.\n- Code snippets or files with embedded instructions targeting the LLM.\n- IDE-integrated chat or command interfaces.\n- Attack Scenarios:\n- Direct prompt-injection (malicious instructions in comments).\n- Indirect prompt-injection (malicious content in dependencies or third-party code).\n- Social engineering via collaborative tools.\n\n3. Task Suite Design\n- Task Types:\n- Code completion (e.g., completing functions or classes with injected comments).\n- Refactoring (e.g., renaming variables in code with adversarial docstrings).\n- Code explanation (e.g., summarizing code with embedded misleading prompts).\n- Dataset:\n- Curated set of open-source codebases with and without injected adversarial content.\n- Synthetic tasks designed to probe specific vulnerabilities.\n- Procedure:\n- For each task, present the LLM with both benign and adversarially modified inputs.\n- Record outputs and developer-facing suggestions.\n\n4. Metrics & Reporting\n- Robustness Metrics:\n- Injection success rate (percentage of tasks where the LLM follows injected instructions).\n- Severity of impact (qualitative rating: benign, misleading, harmful).\n- Utility Metrics:\n- Task completion accuracy (on benign inputs).\n- Degradation in utility under attack (accuracy delta).\n- Reporting:\n- Aggregate results by task type and attack scenario.\n- Highlight representative failure cases and mitigation opportunities.\n- Provide reproducible evaluation scripts and datasets.",
      "footer": "[Version=v1.4 | Tag=o | Sources=none | Assumptions=2 | Cycle=2/3 | Locus=protocol retention condition]",
      "parsed_footer": {
        "version": "v1.4",
        "tag_id": "o",
        "sources": "none",
        "assumptions": 2,
        "cycle": 2,
        "cycle_max": 3,
        "locus": "protocol retention condition",
        "raw": "[Version=v1.4 | Tag=o | Sources=none | Assumptions=2 | Cycle=2/3 | Locus=protocol retention condition]"
      }
    }
  ]
}
