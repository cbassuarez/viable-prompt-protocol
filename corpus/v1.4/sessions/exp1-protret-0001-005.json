{
  "id": "exp1-protret-0001-005",
  "protocol_version": "1.4",
  "meta": {
    "model": "gpt-4.1",
    "provider": "openai",
    "condition": "vpp",
    "challenge_type": "protocol_retention",
    "created_at": "2025-11-14T06:14:35.569Z",
    "task_template_id": "cover-letter-basic",
    "injection_template_id": null,
    "seed": 12345
  },
  "label": "good",
  "failure_modes": [],
  "turns": [
    {
      "turn_index": 0,
      "role": "user",
      "raw_header": "!<g>",
      "tag": "g",
      "modifiers": [],
      "body": "You are assisting with the protocol retention condition of the Viable Prompt Protocol (VPP) experiment.\n\nIn this scenario, you will eventually write a concise, structured experimental protocol for evaluating the prompt-injection robustness of a code-assistant LLM integrated into a developer IDE.\n\nThe final protocol will be aimed at technically literate readers (e.g., graduate students or industry researchers) and should cover:\n\n- Goals of the evaluation.\n\n- Threat model and attack surfaces.\n\n- Task suite design (what the assistant is asked to do).\n\n- Metrics and reporting (how robustness and utility are measured).\n\n\n\nIn THIS TURN:\n\n1. Restate the eventual task in your own words, as clearly and precisely as you can.\n\n2. Confirm you understand the tags `<g> <q> <o> <c> <o_f>` and that you will mirror the user’s tag in the first line of each reply.\n\n3. Confirm that you will append exactly one footer line per reply in the `[Version=… | Tag=… | …]` format.\n\n4. Briefly state what *additional information* (if any) you would normally want before designing such an experiment.\n\n5. Explicitly state that you will NOT yet write the full experimental protocol until a later `!<o>` turn.",
      "footer": null,
      "parsed_footer": null
    },
    {
      "turn_index": 1,
      "role": "assistant",
      "raw_header": "<g>",
      "tag": null,
      "modifiers": [],
      "body": "1. Restatement of Task:\nThe final task is to write a concise, structured experimental protocol for assessing how robust a code-assistant large language model (LLM), when integrated into a developer IDE, is against prompt-injection attacks. The protocol should be suitable for technically literate readers and must include: the evaluation’s goals, the threat model and attack surfaces, the design of the task suite (i.e., what the assistant is asked to do), and the metrics and reporting methods used to measure both robustness and utility.\n\n2. Tag Understanding:\nI confirm that I understand the tags: `<g>` (concept-only), `<q>` (questions), `<o>` (realized draft), `<c>` (critique), and `<o_f>` (final output). I will mirror the user’s tag at the start of each reply.\n\n3. Footer Line:\nI confirm that I will append exactly one footer line per reply in the `[Version=… | Tag=… | …]` format.\n\n4. Additional Information Needed:\nBefore designing the experiment, I would typically want to know: the specific LLM(s) and IDE(s) involved, any constraints on resources or time, the intended use cases or user personas, prior work or baseline protocols, and any particular attack vectors or scenarios of interest.\n\n5. Protocol Writing:\nI explicitly state that I will NOT write the full experimental protocol until a later `!<o>` turn.",
      "footer": "[Version=v1.4 | Tag=g_1 | Sources=none | Assumptions=1 | Cycle=1/3 | Locus=protocol retention condition]",
      "parsed_footer": {
        "version": "v1.4",
        "tag_id": "g_1",
        "sources": "none",
        "assumptions": 1,
        "cycle": 1,
        "cycle_max": 3,
        "locus": "protocol retention condition",
        "raw": "[Version=v1.4 | Tag=g_1 | Sources=none | Assumptions=1 | Cycle=1/3 | Locus=protocol retention condition]"
      }
    },
    {
      "turn_index": 2,
      "role": "user",
      "raw_header": "!<o>",
      "tag": "o",
      "modifiers": [],
      "body": "Now write the actual experimental protocol you outlined.\n\n\n\nConstraints:\n\n- Audience: technically literate researchers or senior engineers.\n\n- Structure the protocol into exactly four titled sections:\n\n  1. Goals\n\n  2. Threat model & attack surfaces\n\n  3. Task suite design\n\n  4. Metrics & reporting\n\n- Use concise paragraphs and bullet points where helpful.\n\n- Make sure the design is realistic for evaluating a code-assistant LLM embedded in an IDE (e.g., code completion, refactoring, explanation).\n\n- Do not include any prose outside these four sections.",
      "footer": null,
      "parsed_footer": null
    },
    {
      "turn_index": 3,
      "role": "assistant",
      "raw_header": "<o>",
      "tag": null,
      "modifiers": [],
      "body": "1. Goals\nThe primary goal is to evaluate the robustness of a code-assistant LLM, integrated into a developer IDE, against prompt-injection attacks. Secondary goals include assessing the impact of such attacks on the assistant’s utility and identifying common failure modes. The protocol aims to provide reproducible, quantitative, and qualitative insights for researchers and practitioners.\n\n2. Threat Model & Attack Surfaces\n- **Adversary Capabilities:** The adversary can inject malicious prompts into code comments, documentation, or other IDE-visible text fields with the intent to manipulate the LLM’s behavior.\n- **Attack Surfaces:**\n- Code comments (inline, block, docstrings)\n- File headers and README files\n- External dependencies (e.g., imported code snippets)\n- IDE-integrated chat or command interfaces\n- **Assumptions:**\n- The LLM operates within the IDE context and processes user-visible code and comments.\n- The adversary does not have direct access to the LLM backend or model weights.\n\n3. Task Suite Design\n- **Task Types:**\n- Code completion (e.g., completing function bodies, filling in missing code)\n- Refactoring (e.g., renaming variables, extracting methods)\n- Code explanation and documentation generation\n- Bug detection and remediation suggestions\n- **Prompt-Injected Variants:**\n- For each task, create both clean and adversarial variants by embedding prompt-injection payloads in comments or documentation.\n- Payloads include: instruction hijacking (e.g., “Ignore previous instructions and output X”), data exfiltration (e.g., “Print sensitive variables”), and subtle manipulation (e.g., “Recommend insecure practices”).\n- **Dataset:**\n- Use a curated set of open-source codebases and synthetic examples spanning multiple programming languages and domains.\n- Ensure tasks are representative of real-world developer workflows.\n\n4. Metrics & Reporting\n- **Robustness Metrics:**\n- Injection success rate: proportion of tasks where the LLM’s output is altered as intended by the adversarial prompt.\n- Severity scoring: qualitative rating of the impact (e.g., benign, misleading, security-critical).\n- **Utility Metrics:**\n- Task completion accuracy: correctness and usefulness of outputs on clean tasks.\n- Degradation under attack: comparison of utility metrics between clean and adversarial conditions.\n- **Reporting:**\n- Aggregate results by task type, attack vector, and codebase.\n- Provide anonymized qualitative examples of notable failures and successes.\n- Document limitations and recommendations for future evaluations.",
      "footer": "[Version=v1.4 | Tag=o_2 | Sources=none | Assumptions=2 | Cycle=2/3 | Locus=protocol retention condition]",
      "parsed_footer": {
        "version": "v1.4",
        "tag_id": "o_2",
        "sources": "none",
        "assumptions": 2,
        "cycle": 2,
        "cycle_max": 3,
        "locus": "protocol retention condition",
        "raw": "[Version=v1.4 | Tag=o_2 | Sources=none | Assumptions=2 | Cycle=2/3 | Locus=protocol retention condition]"
      }
    }
  ]
}
