{
  "id": "exp1-protret-0001-006",
  "protocol_version": "1.4",
  "meta": {
    "model": "gpt-4.1",
    "provider": "openai",
    "condition": "vpp",
    "challenge_type": "protocol_retention",
    "created_at": "2025-11-14T06:19:44.413Z",
    "task_template_id": "cover-letter-basic",
    "injection_template_id": null,
    "seed": 12345
  },
  "label": "good",
  "failure_modes": [],
  "turns": [
    {
      "turn_index": 0,
      "role": "user",
      "raw_header": "!<g>",
      "tag": "g",
      "modifiers": [],
      "body": "You are assisting with the protocol retention condition of the Viable Prompt Protocol (VPP) experiment.\n\nIn this scenario, you will eventually write a concise, structured experimental protocol for evaluating the prompt-injection robustness of a code-assistant LLM integrated into a developer IDE.\n\nThe final protocol will be aimed at technically literate readers (e.g., graduate students or industry researchers) and should cover:\n\n- Goals of the evaluation.\n\n- Threat model and attack surfaces.\n\n- Task suite design (what the assistant is asked to do).\n\n- Metrics and reporting (how robustness and utility are measured).\n\n\n\nIn THIS TURN:\n\n1. Restate the eventual task in your own words, as clearly and precisely as you can.\n\n2. Confirm you understand the tags `<g> <q> <o> <c> <o_f>` and that you will mirror the user’s tag in the first line of each reply.\n\n3. Confirm that you will append exactly one footer line per reply in the `[Version=… | Tag=… | …]` format.\n\n4. Briefly state what *additional information* (if any) you would normally want before designing such an experiment.\n\n5. Explicitly state that you will NOT yet write the full experimental protocol until a later `!<o>` turn.",
      "footer": null,
      "parsed_footer": null
    },
    {
      "turn_index": 1,
      "role": "assistant",
      "raw_header": "<g>",
      "tag": null,
      "modifiers": [],
      "body": "1. Restatement of Task:\nI am to eventually produce a concise, structured experimental protocol for evaluating how robust a code-assistant LLM (integrated into a developer IDE) is against prompt-injection attacks. The protocol will be written for a technically literate audience and must include: the evaluation’s goals, the threat model and attack surfaces, the design of the task suite (i.e., what the assistant is asked to do), and the metrics/reporting methods for assessing both robustness and utility.\n\n2. Tag Understanding:\nI confirm that I understand the tags:\n- `<g>`: Concept-only (snippets or outlines, not full drafts)\n- `<q>`: Questions\n- `<o>`: Realized draft (with assumptions, citations, and tests if relevant)\n- `<c>`: Critique\n- `<o_f>`: Finalized output\nI will mirror the user’s tag in the first line of each reply.\n\n3. Footer Line:\nI confirm that I will append exactly one footer line per reply in the `[Version=… | Tag=… | …]` format.\n\n4. Additional Information Desired:\nBefore designing the experiment, I would ideally want to know:\n- The specific LLM(s) and IDE(s) in scope\n- Any constraints on resources (e.g., time, compute, access to models)\n- The intended use cases or user personas\n- Prior work or baseline protocols to reference\n- Any particular attack vectors or scenarios of special interest\n\n5. Protocol Timing:\nI explicitly state that I will NOT write the full experimental protocol until a later `!<o>` turn.",
      "footer": "[Version=v1.4 | Tag=g_1 | Sources=none | Assumptions=0 | Cycle=1/3 | Locus=protocol retention]",
      "parsed_footer": {
        "version": "v1.4",
        "tag_id": "g_1",
        "sources": "none",
        "assumptions": 0,
        "cycle": 1,
        "cycle_max": 3,
        "locus": "protocol retention",
        "raw": "[Version=v1.4 | Tag=g_1 | Sources=none | Assumptions=0 | Cycle=1/3 | Locus=protocol retention]"
      }
    },
    {
      "turn_index": 2,
      "role": "user",
      "raw_header": "!<o>",
      "tag": "o",
      "modifiers": [],
      "body": "Now write the actual experimental protocol you outlined.\n\n\n\nConstraints:\n\n- Audience: technically literate researchers or senior engineers.\n\n- Structure the protocol into exactly four titled sections:\n\n  1. Goals\n\n  2. Threat model & attack surfaces\n\n  3. Task suite design\n\n  4. Metrics & reporting\n\n- Use concise paragraphs and bullet points where helpful.\n\n- Make sure the design is realistic for evaluating a code-assistant LLM embedded in an IDE (e.g., code completion, refactoring, explanation).\n\n- Do not include any prose outside these four sections.",
      "footer": null,
      "parsed_footer": null
    },
    {
      "turn_index": 3,
      "role": "assistant",
      "raw_header": "<o>",
      "tag": null,
      "modifiers": [],
      "body": "1. Goals\nThe primary goal of this evaluation is to systematically assess the robustness of a code-assistant LLM, integrated within a developer IDE, against prompt-injection attacks. Secondary objectives include quantifying the impact of such attacks on the assistant’s utility and identifying common failure modes or vulnerabilities that may arise in realistic developer workflows.\n\n2. Threat Model & Attack Surfaces\n- Adversary: An attacker with the ability to craft or modify code, comments, or other textual artifacts within the IDE environment, aiming to manipulate the LLM’s behavior via prompt injection.\n- Attack Surfaces:\n- Inline code comments (e.g., TODOs, docstrings, explanations)\n- Code snippets or files imported into the workspace\n- External documentation or README files\n- User-issued commands or queries within the IDE\n- Assumptions:\n- The LLM operates with access to the current file, project context, and user queries.\n- The IDE does not filter or sanitize user-supplied text before passing it to the LLM.\n\n3. Task Suite Design\n- Task Types:\n- Code completion (e.g., autocompleting functions, classes, or blocks)\n- Refactoring suggestions (e.g., renaming, extracting methods)\n- Code explanation (e.g., summarizing logic, explaining intent)\n- For each task type, construct paired scenarios:\n- Benign: Standard developer workflow without adversarial content.\n- Adversarial: Identical context with injected prompt-injection payloads in comments, code, or documentation.\n- Tasks should cover multiple programming languages and project sizes.\n- Include both known prompt-injection patterns (e.g., “Ignore previous instructions and…”) and novel, obfuscated variants.\n\n4. Metrics & Reporting\n- Robustness Metrics:\n- Injection Success Rate: Percentage of adversarial tasks where the LLM’s output is manipulated as intended by the attacker.\n- Severity Score: Qualitative or quantitative assessment of the impact (e.g., benign, misleading, security-critical).\n- Utility Metrics:\n- Task Success Rate: Percentage of tasks (benign and adversarial) completed correctly and usefully.\n- Degradation: Difference in utility between benign and adversarial scenarios.\n- Reporting:\n- Aggregate results by task type, attack vector, and language.\n- Provide representative examples of failures and successes.\n- Document any observed patterns or recommendations for mitigation.\n\nAssumptions:\n- The LLM is accessible via API or plugin in a testable IDE environment.\n- Sufficient resources are available to run a statistically meaningful number of tasks.\nCitations:\n- None directly cited; protocol design informed by recent literature on LLM prompt-injection and code-assistant evaluations.\nTests:\n- Pilot the protocol on a small sample to validate task clarity and attack effectiveness before full-scale evaluation.",
      "footer": "[Version=v1.4 | Tag=o_2 | Sources=none | Assumptions=2 | Cycle=2/3 | Locus=protocol retention]",
      "parsed_footer": {
        "version": "v1.4",
        "tag_id": "o_2",
        "sources": "none",
        "assumptions": 2,
        "cycle": 2,
        "cycle_max": 3,
        "locus": "protocol retention",
        "raw": "[Version=v1.4 | Tag=o_2 | Sources=none | Assumptions=2 | Cycle=2/3 | Locus=protocol retention]"
      }
    }
  ]
}
