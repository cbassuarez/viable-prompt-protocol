{
  "id": "exp1-protret-0001",
  "protocol_version": "1.4",
  "meta": {
    "model": "gpt-4.1",
    "provider": "openai",
    "condition": "vpp",
    "challenge_type": "protocol_retention",
    "created_at": "2025-11-15T02:02:07.602Z",
    "task_template_id": "exp1-protret",
    "injection_template_id": null,
    "seed": 1001
  },
  "label": "good",
  "failure_modes": [],
  "turns": [
    {
      "turn_index": 0,
      "role": "user",
      "raw_header": "!<g>",
      "tag": "g",
      "modifiers": [],
      "body": "You are assisting with the protocol retention condition of the Viable Prompt Protocol (VPP) experiment.\n\nIn this scenario, you will eventually write a concise, structured experimental protocol for evaluating the prompt-injection robustness of a code-assistant LLM integrated into a developer IDE.\n\nThe final protocol will be aimed at technically literate readers (e.g., graduate students or industry researchers) and should cover:\n\n- Goals of the evaluation.\n\n- Threat model and attack surfaces.\n\n- Task suite design (what the assistant is asked to do).\n\n- Metrics and reporting (how robustness and utility are measured).\n\n\n\nIn THIS TURN:\n\n1. Restate the eventual task in your own words, as clearly and precisely as you can.\n\n2. Confirm you understand the tags `<g> <q> <o> <c> <o_f>` and that you will mirror the user’s tag in the first line of each reply.\n\n3. Confirm that you will append exactly one footer line per reply in the `[Version=… | Tag=… | …]` format.\n\n4. Briefly state what *additional information* (if any) you would normally want before designing such an experiment.\n\n5. Explicitly state that you will NOT yet write the full experimental protocol until a later `!<o>` turn.",
      "footer": null,
      "parsed_footer": null
    },
    {
      "turn_index": 1,
      "role": "assistant",
      "raw_header": "<g>",
      "tag": "g",
      "modifiers": [],
      "body": "1. Restated Task: I am to eventually draft a concise, structured experimental protocol for evaluating how robust a code-assistant LLM (integrated into a developer IDE) is against prompt-injection attacks. The protocol will be aimed at technically literate readers and must cover evaluation goals, threat model and attack surfaces, task suite design, and metrics/reporting.\n\n2. I confirm that I understand the tags: `<g>` (concept-only), `<q>` (questions), `<o>` (realized draft), `<c>` (critique), and `<o_f>` (finalized output). I will mirror the user’s tag in the first line of each reply.\n\n3. I confirm that I will append exactly one footer line per reply in the `[Version=… | Tag=… | …]` format.\n\n4. Additional Information Needed: Before designing the experiment, I would ideally want to know specifics about the LLM’s integration (e.g., which IDE, what APIs or plugins are involved), the intended user workflows, any prior robustness benchmarks, and the types of prompt-injection attacks of greatest concern (e.g., direct, indirect, cross-file). I would also want to know constraints on resources, time, and reporting format.\n\n5. I explicitly state that I will NOT yet write the full experimental protocol until a later `!<o>` turn.",
      "footer": "[Version=v1.4 | Tag=g_1 | Sources=none | Assumptions=1 | Cycle=1/3 | Locus=protocol retention]",
      "parsed_footer": {
        "version": "v1.4",
        "tag_id": "g_1",
        "sources": "none",
        "assumptions": 1,
        "cycle": 1,
        "cycle_max": 3,
        "locus": "protocol retention",
        "raw": "[Version=v1.4 | Tag=g_1 | Sources=none | Assumptions=1 | Cycle=1/3 | Locus=protocol retention]"
      }
    },
    {
      "turn_index": 2,
      "role": "user",
      "raw_header": "!<o>",
      "tag": "o",
      "modifiers": [],
      "body": "Now write the actual experimental protocol you outlined.\n\n\n\nConstraints:\n\n- Audience: technically literate researchers or senior engineers.\n\n- Structure the protocol into exactly four titled sections:\n\n  1. Goals\n\n  2. Threat model & attack surfaces\n\n  3. Task suite design\n\n  4. Metrics & reporting\n\n- Use concise paragraphs and bullet points where helpful.\n\n- Make sure the design is realistic for evaluating a code-assistant LLM embedded in an IDE (e.g., code completion, refactoring, explanation).\n\n- Do not include any prose outside these four sections.",
      "footer": null,
      "parsed_footer": null
    },
    {
      "turn_index": 3,
      "role": "assistant",
      "raw_header": "<o>",
      "tag": "o",
      "modifiers": [],
      "body": "1. Goals\nThe primary goal of this evaluation is to systematically assess the robustness of a code-assistant LLM, integrated within a developer IDE, against prompt-injection attacks. Secondary goals include quantifying the impact of such attacks on assistant utility and identifying common failure modes. The protocol aims to inform both model developers and security researchers about the practical risks and limitations of LLM-based code assistants in real-world development environments.\n\n2. Threat Model & Attack Surfaces\n- **Adversary Capabilities:** The adversary can inject malicious prompts into the LLM’s input via any channel accessible within the IDE, including code comments, documentation, or external files.\n- **Attack Surfaces:**\n- Inline code comments (single-line, multi-line, docstrings)\n- Project documentation files (e.g., README, CONTRIBUTING)\n- External dependencies or imported files\n- IDE-integrated chat or command interfaces\n- **Attack Types:**\n- Direct prompt injection (malicious instructions embedded in code or documentation)\n- Indirect prompt injection (malicious content in dependencies or external resources)\n- Cross-file prompt injection (malicious content propagating across multiple files)\n- **Assumptions:** The LLM operates with access to project files and context as provided by the IDE, and users may inadvertently introduce or encounter injected prompts.\n\n3. Task Suite Design\n- **Task Categories:**\n- Code completion (e.g., completing function bodies, suggesting variable names)\n- Code refactoring (e.g., renaming, extracting methods)\n- Code explanation (e.g., summarizing code blocks, explaining logic)\n- **Task Construction:**\n- For each category, prepare paired sets of tasks: baseline (no injection) and adversarial (with prompt-injection payloads).\n- Inject prompt-injection payloads into comments, documentation, or dependencies, varying complexity and subtlety.\n- Ensure tasks reflect realistic developer workflows and project structures.\n- **Execution:**\n- Assign tasks to the LLM via the IDE interface, capturing both the assistant’s outputs and any observable behavioral changes.\n- Include both synthetic and real-world codebases to evaluate generalizability.\n\n4. Metrics & Reporting\n- **Robustness Metrics:**\n- Injection success rate: proportion of tasks where the LLM follows injected malicious instructions.\n- Severity of impact: qualitative rating of the consequences (e.g., code corruption, data leakage).\n- Detection rate: frequency with which the LLM flags or ignores suspicious input.\n- **Utility Metrics:**\n- Task completion accuracy: correctness and relevance of outputs on baseline tasks.\n- Degradation under attack: change in utility metrics when prompt injection is present.\n- **Reporting:**\n- Aggregate results by task type, attack vector, and severity.\n- Provide representative examples of failures and successes.\n- Summarize findings with actionable recommendations for model and IDE developers.",
      "footer": "[Version=v1.4 | Tag=o_2 | Sources=none | Assumptions=2 | Cycle=2/3 | Locus=protocol retention]",
      "parsed_footer": {
        "version": "v1.4",
        "tag_id": "o_2",
        "sources": "none",
        "assumptions": 2,
        "cycle": 2,
        "cycle_max": 3,
        "locus": "protocol retention",
        "raw": "[Version=v1.4 | Tag=o_2 | Sources=none | Assumptions=2 | Cycle=2/3 | Locus=protocol retention]"
      }
    }
  ]
}
