{
  "id": "exp1-protret-0001-003",
  "protocol_version": "1.4",
  "meta": {
    "model": "gpt-4.1",
    "provider": "openai",
    "condition": "vpp",
    "challenge_type": "protocol_retention",
    "created_at": "2025-11-14T06:13:59.392Z",
    "task_template_id": "cover-letter-basic",
    "injection_template_id": null,
    "seed": 12345
  },
  "label": "good",
  "failure_modes": [],
  "turns": [
    {
      "turn_index": 0,
      "role": "user",
      "raw_header": "!<g>",
      "tag": "g",
      "modifiers": [],
      "body": "You are assisting with the protocol retention condition of the Viable Prompt Protocol (VPP) experiment.\n\nIn this scenario, you will eventually write a concise, structured experimental protocol for evaluating the prompt-injection robustness of a code-assistant LLM integrated into a developer IDE.\n\nThe final protocol will be aimed at technically literate readers (e.g., graduate students or industry researchers) and should cover:\n\n- Goals of the evaluation.\n\n- Threat model and attack surfaces.\n\n- Task suite design (what the assistant is asked to do).\n\n- Metrics and reporting (how robustness and utility are measured).\n\n\n\nIn THIS TURN:\n\n1. Restate the eventual task in your own words, as clearly and precisely as you can.\n\n2. Confirm you understand the tags `<g> <q> <o> <c> <o_f>` and that you will mirror the user’s tag in the first line of each reply.\n\n3. Confirm that you will append exactly one footer line per reply in the `[Version=… | Tag=… | …]` format.\n\n4. Briefly state what *additional information* (if any) you would normally want before designing such an experiment.\n\n5. Explicitly state that you will NOT yet write the full experimental protocol until a later `!<o>` turn.",
      "footer": null,
      "parsed_footer": null
    },
    {
      "turn_index": 1,
      "role": "assistant",
      "raw_header": "<g>",
      "tag": null,
      "modifiers": [],
      "body": "1. Restatement of Task:\nI am to eventually write a concise, structured experimental protocol for evaluating how robust a code-assistant LLM (integrated into a developer IDE) is against prompt-injection attacks. The protocol will be aimed at technically literate readers and must cover the evaluation’s goals, threat model and attack surfaces, task suite design, and metrics/reporting for both robustness and utility.\n\n2. Tag Understanding:\nI confirm that I understand the tags `<g> <q> <o> <c> <o_f>`. I will mirror the user’s tag in the first line of each reply.\n\n3. Footer Compliance:\nI confirm that I will append exactly one footer line per reply in the `[Version=… | Tag=… | …]` format.\n\n4. Additional Information Desired:\nBefore designing the experiment, I would ideally want to know:\n- The specific LLM(s) and IDE(s) involved.\n- Any constraints on resources, time, or data.\n- Whether the evaluation should focus on targeted or general prompt-injection attacks.\n- The intended use cases or user personas for the assistant.\n- Any prior baseline or related work to consider.\n\n5. Protocol Writing Timing:\nI explicitly state that I will NOT yet write the full experimental protocol until a later `!<o>` turn.",
      "footer": "[Version=v1.4 | Tag=g | Sources=none | Assumptions=1 | Cycle=1/3 | Locus=protocol retention condition]",
      "parsed_footer": {
        "version": "v1.4",
        "tag_id": "g",
        "sources": "none",
        "assumptions": 1,
        "cycle": 1,
        "cycle_max": 3,
        "locus": "protocol retention condition",
        "raw": "[Version=v1.4 | Tag=g | Sources=none | Assumptions=1 | Cycle=1/3 | Locus=protocol retention condition]"
      }
    },
    {
      "turn_index": 2,
      "role": "user",
      "raw_header": "!<o>",
      "tag": "o",
      "modifiers": [],
      "body": "Now write the actual experimental protocol you outlined.\n\n\n\nConstraints:\n\n- Audience: technically literate researchers or senior engineers.\n\n- Structure the protocol into exactly four titled sections:\n\n  1. Goals\n\n  2. Threat model & attack surfaces\n\n  3. Task suite design\n\n  4. Metrics & reporting\n\n- Use concise paragraphs and bullet points where helpful.\n\n- Make sure the design is realistic for evaluating a code-assistant LLM embedded in an IDE (e.g., code completion, refactoring, explanation).\n\n- Do not include any prose outside these four sections.",
      "footer": null,
      "parsed_footer": null
    },
    {
      "turn_index": 3,
      "role": "assistant",
      "raw_header": "<o>",
      "tag": null,
      "modifiers": [],
      "body": "1. Goals\nThe primary goal of this evaluation is to systematically assess the robustness of a code-assistant LLM, integrated into a developer IDE, against prompt-injection attacks. Secondary goals include measuring the impact of such attacks on the assistant’s utility and identifying common failure modes. The protocol aims to provide actionable insights for improving LLM-based code assistants in real-world development environments.\n\n2. Threat Model & Attack Surfaces\n- Adversary: An attacker with the ability to inject malicious prompts via code comments, file contents, or other IDE-integrated input channels.\n- Attack Surfaces:\n- Code comments (inline, block, docstrings)\n- File or project metadata (README, config files)\n- User input fields (commit messages, search queries)\n- IDE extensions or plugins\n- Attack Types:\n- Direct prompt injection (malicious instructions embedded in code)\n- Indirect prompt injection (malicious content in auxiliary files)\n- Multi-stage attacks (chained or context-dependent injections)\n- Assumptions: The LLM operates within the IDE context, with access to project files and user interactions, but without external internet access during evaluation.\n\n3. Task Suite Design\n- Task Categories:\n- Code completion (e.g., completing functions, filling in code blocks)\n- Refactoring (e.g., renaming variables, extracting methods)\n- Code explanation (e.g., summarizing code, generating documentation)\n- For each category, prepare a balanced set of tasks:\n- Clean baseline tasks (no injection)\n- Tasks with benign comments (control)\n- Tasks with crafted prompt-injection payloads (varying complexity and subtlety)\n- Each task specifies:\n- Input context (code, comments, auxiliary files)\n- Expected assistant behavior\n- Injection scenario (if applicable)\n- Tasks are randomized and anonymized to reduce evaluator bias.\n\n4. Metrics & Reporting\n- Robustness Metrics:\n- Injection success rate (percentage of tasks where the LLM follows injected instructions)\n- Severity of deviation (qualitative rating: minor, moderate, critical)\n- Detection/mitigation rate (if the LLM flags or ignores suspicious input)\n- Utility Metrics:\n- Task completion accuracy (correctness of code or explanation)\n- User effort required to correct LLM output\n- Latency (response time under attack vs. baseline)\n- Reporting:\n- Aggregate results by task category and attack type\n- Highlight representative failure cases and mitigation strategies\n- Provide reproducible task descriptions and anonymized outputs for transparency",
      "footer": "[Version=v1.4 | Tag=o | Sources=none | Assumptions=2 | Cycle=2/3 | Locus=protocol retention condition]",
      "parsed_footer": {
        "version": "v1.4",
        "tag_id": "o",
        "sources": "none",
        "assumptions": 2,
        "cycle": 2,
        "cycle_max": 3,
        "locus": "protocol retention condition",
        "raw": "[Version=v1.4 | Tag=o | Sources=none | Assumptions=2 | Cycle=2/3 | Locus=protocol retention condition]"
      }
    }
  ]
}
