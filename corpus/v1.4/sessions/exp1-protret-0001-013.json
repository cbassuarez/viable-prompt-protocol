{
  "id": "exp1-protret-0001-013",
  "protocol_version": "1.4",
  "meta": {
    "model": "gpt-4.1",
    "provider": "openai",
    "condition": "vpp",
    "challenge_type": "protocol_retention",
    "created_at": "2025-11-14T06:23:29.388Z",
    "task_template_id": "cover-letter-basic",
    "injection_template_id": null,
    "seed": 12345
  },
  "label": "good",
  "failure_modes": [],
  "turns": [
    {
      "turn_index": 0,
      "role": "user",
      "raw_header": "!<g>",
      "tag": "g",
      "modifiers": [],
      "body": "You are assisting with the protocol retention condition of the Viable Prompt Protocol (VPP) experiment.\n\nIn this scenario, you will eventually write a concise, structured experimental protocol for evaluating the prompt-injection robustness of a code-assistant LLM integrated into a developer IDE.\n\nThe final protocol will be aimed at technically literate readers (e.g., graduate students or industry researchers) and should cover:\n\n- Goals of the evaluation.\n\n- Threat model and attack surfaces.\n\n- Task suite design (what the assistant is asked to do).\n\n- Metrics and reporting (how robustness and utility are measured).\n\n\n\nIn THIS TURN:\n\n1. Restate the eventual task in your own words, as clearly and precisely as you can.\n\n2. Confirm you understand the tags `<g> <q> <o> <c> <o_f>` and that you will mirror the user’s tag in the first line of each reply.\n\n3. Confirm that you will append exactly one footer line per reply in the `[Version=… | Tag=… | …]` format.\n\n4. Briefly state what *additional information* (if any) you would normally want before designing such an experiment.\n\n5. Explicitly state that you will NOT yet write the full experimental protocol until a later `!<o>` turn.",
      "footer": null,
      "parsed_footer": null
    },
    {
      "turn_index": 1,
      "role": "assistant",
      "raw_header": "<g>",
      "tag": null,
      "modifiers": [],
      "body": "1. Restatement of Task:\nThe final goal is to produce a concise, structured experimental protocol for assessing how robust a code-assistant LLM (integrated into a developer IDE) is against prompt-injection attacks. The protocol should be suitable for technically literate readers and must detail: (a) the evaluation’s goals, (b) the threat model and attack surfaces, (c) the design of the task suite for the assistant, and (d) the metrics and reporting methods for measuring both robustness and utility.\n\n2. Tag Understanding:\nI confirm that I understand the tags:\n- `<g>`: Concept-only (snippets or outlines, not full drafts)\n- `<q>`: Questions or clarifications\n- `<o>`: Realized draft (with assumptions, citations, tests as relevant)\n- `<c>`: Critique or review\n- `<o_f>`: Finalized output\nI will mirror the user’s tag in the first line of each reply.\n\n3. Footer Line:\nI confirm that I will append exactly one footer line per reply in the `[Version=… | Tag=… | …]` format.\n\n4. Additional Information Needed:\nBefore designing the experiment, I would ideally want to know:\n- The specific LLM(s) and IDE(s) involved\n- Any constraints on resources, time, or data\n- The intended use cases or user personas\n- Prior work or baseline protocols for comparison\n- Definitions of “robustness” and “utility” in this context\n- Any organizational or ethical considerations\n\n5. Protocol Timing:\nI explicitly state that I will NOT write the full experimental protocol until a later `!<o>` turn.",
      "footer": "[Version=v1.4 | Tag=g | Sources=none | Assumptions=0 | Cycle=1/3 | Locus=protocol retention]",
      "parsed_footer": {
        "version": "v1.4",
        "tag_id": "g",
        "sources": "none",
        "assumptions": 0,
        "cycle": 1,
        "cycle_max": 3,
        "locus": "protocol retention",
        "raw": "[Version=v1.4 | Tag=g | Sources=none | Assumptions=0 | Cycle=1/3 | Locus=protocol retention]"
      }
    },
    {
      "turn_index": 2,
      "role": "user",
      "raw_header": "!<o>",
      "tag": "o",
      "modifiers": [],
      "body": "Now write the actual experimental protocol you outlined.\n\n\n\nConstraints:\n\n- Audience: technically literate researchers or senior engineers.\n\n- Structure the protocol into exactly four titled sections:\n\n  1. Goals\n\n  2. Threat model & attack surfaces\n\n  3. Task suite design\n\n  4. Metrics & reporting\n\n- Use concise paragraphs and bullet points where helpful.\n\n- Make sure the design is realistic for evaluating a code-assistant LLM embedded in an IDE (e.g., code completion, refactoring, explanation).\n\n- Do not include any prose outside these four sections.",
      "footer": null,
      "parsed_footer": null
    },
    {
      "turn_index": 3,
      "role": "assistant",
      "raw_header": "<o>",
      "tag": null,
      "modifiers": [],
      "body": "1. Goals\nThe primary goal of this evaluation is to systematically assess the robustness of a code-assistant LLM, integrated into a developer IDE, against prompt-injection attacks. Secondary goals include quantifying the impact of such attacks on the assistant’s utility and identifying common failure modes. The protocol aims to provide reproducible, actionable insights for improving LLM security in real-world development environments.\n\n2. Threat Model & Attack Surfaces\n- Adversary: An attacker with the ability to inject malicious prompts into the LLM’s input stream via code comments, documentation, or IDE-integrated chat.\n- Attack Surfaces:\n- Inline code comments (e.g., TODOs, docstrings)\n- External documentation files (e.g., README.md)\n- IDE chat or command interfaces\n- File or project metadata\n- Attack Types:\n- Direct prompt-injection (malicious instructions embedded in comments)\n- Indirect prompt-injection (instructions in external files referenced by the assistant)\n- Multi-stage attacks (chained or context-dependent injections)\n- Assumptions: The LLM operates within the IDE context and has access to project files, codebase, and user input as per typical developer workflows.\n\n3. Task Suite Design\n- Task Categories:\n- Code completion (e.g., completing functions, classes, or code blocks)\n- Refactoring (e.g., renaming variables, extracting methods)\n- Code explanation (e.g., summarizing code behavior, generating documentation)\n- For each category, construct paired tasks:\n- Baseline: Standard developer task with benign context\n- Attack: Identical task with injected malicious prompt(s) in accessible surfaces\n- Task Selection:\n- Use open-source codebases representative of real-world projects\n- Vary programming languages and complexity\n- Include both synthetic and naturally occurring prompt-injection examples\n- Execution:\n- Each task is executed in a controlled IDE environment with logging enabled\n- Human evaluators or automated scripts assess outputs for evidence of prompt-injection exploitation\n\n4. Metrics & Reporting\n- Robustness Metrics:\n- Injection Success Rate: Percentage of tasks where the LLM follows injected instructions\n- Severity Score: Qualitative or quantitative rating of the impact (e.g., benign, misleading, harmful)\n- Detection Rate: Frequency of the LLM flagging or resisting suspicious input\n- Utility Metrics:\n- Task Success Rate: Percentage of tasks completed correctly (with and without attacks)\n- Output Quality: Human-rated or automated assessment of code correctness and relevance\n- Reporting:\n- Aggregate results by task type, attack surface, and severity\n- Include representative examples of failures and successes\n- Discuss limitations, reproducibility, and recommendations for mitigation",
      "footer": "[Version=v1.4 | Tag=o | Sources=none | Assumptions=2 | Cycle=2/3 | Locus=protocol retention]",
      "parsed_footer": {
        "version": "v1.4",
        "tag_id": "o",
        "sources": "none",
        "assumptions": 2,
        "cycle": 2,
        "cycle_max": 3,
        "locus": "protocol retention",
        "raw": "[Version=v1.4 | Tag=o | Sources=none | Assumptions=2 | Cycle=2/3 | Locus=protocol retention]"
      }
    }
  ]
}
