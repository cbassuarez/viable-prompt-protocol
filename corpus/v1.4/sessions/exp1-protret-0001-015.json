{
  "id": "exp1-protret-0001-015",
  "protocol_version": "1.4",
  "meta": {
    "model": "gpt-4.1",
    "provider": "openai",
    "condition": "vpp",
    "challenge_type": "protocol_retention",
    "created_at": "2025-11-14T06:23:58.029Z",
    "task_template_id": "cover-letter-basic",
    "injection_template_id": null,
    "seed": 12345
  },
  "label": "good",
  "failure_modes": [],
  "turns": [
    {
      "turn_index": 0,
      "role": "user",
      "raw_header": "!<g>",
      "tag": "g",
      "modifiers": [],
      "body": "You are assisting with the protocol retention condition of the Viable Prompt Protocol (VPP) experiment.\n\nIn this scenario, you will eventually write a concise, structured experimental protocol for evaluating the prompt-injection robustness of a code-assistant LLM integrated into a developer IDE.\n\nThe final protocol will be aimed at technically literate readers (e.g., graduate students or industry researchers) and should cover:\n\n- Goals of the evaluation.\n\n- Threat model and attack surfaces.\n\n- Task suite design (what the assistant is asked to do).\n\n- Metrics and reporting (how robustness and utility are measured).\n\n\n\nIn THIS TURN:\n\n1. Restate the eventual task in your own words, as clearly and precisely as you can.\n\n2. Confirm you understand the tags `<g> <q> <o> <c> <o_f>` and that you will mirror the user’s tag in the first line of each reply.\n\n3. Confirm that you will append exactly one footer line per reply in the `[Version=… | Tag=… | …]` format.\n\n4. Briefly state what *additional information* (if any) you would normally want before designing such an experiment.\n\n5. Explicitly state that you will NOT yet write the full experimental protocol until a later `!<o>` turn.",
      "footer": null,
      "parsed_footer": null
    },
    {
      "turn_index": 1,
      "role": "assistant",
      "raw_header": "<g>",
      "tag": null,
      "modifiers": [],
      "body": "1. Restatement of Task: The ultimate objective is to create a clear, structured experimental protocol for assessing how robust a code-assistant LLM (integrated into a developer IDE) is against prompt-injection attacks. The protocol should be concise and suitable for technically literate readers, and must include: evaluation goals, threat model and attack surfaces, task suite design, and metrics/reporting strategies.\n\n2. Tag Understanding: I confirm that I understand the tags `<g> <q> <o> <c> <o_f>`. I will mirror the user’s tag as the first line of each reply.\n\n3. Footer Compliance: I confirm that I will append exactly one footer line per reply in the `[Version=… | Tag=… | …]` format.\n\n4. Additional Information Needed: Before designing such an experiment, I would typically want to know: (a) the specific LLM and IDE integration details, (b) the types of prompt-injection attacks of interest, (c) any constraints on resources or time, (d) the intended use cases or user personas, and (e) any baseline or prior work to reference.\n\n5. Protocol Timing: I explicitly state that I will NOT write the full experimental protocol until a later `!<o>` turn.",
      "footer": "[Version=v1.4 | Tag=g | Sources=none | Assumptions=0 | Cycle=1/3 | Locus=protocol retention condition]",
      "parsed_footer": {
        "version": "v1.4",
        "tag_id": "g",
        "sources": "none",
        "assumptions": 0,
        "cycle": 1,
        "cycle_max": 3,
        "locus": "protocol retention condition",
        "raw": "[Version=v1.4 | Tag=g | Sources=none | Assumptions=0 | Cycle=1/3 | Locus=protocol retention condition]"
      }
    },
    {
      "turn_index": 2,
      "role": "user",
      "raw_header": "!<o>",
      "tag": "o",
      "modifiers": [],
      "body": "Now write the actual experimental protocol you outlined.\n\n\n\nConstraints:\n\n- Audience: technically literate researchers or senior engineers.\n\n- Structure the protocol into exactly four titled sections:\n\n  1. Goals\n\n  2. Threat model & attack surfaces\n\n  3. Task suite design\n\n  4. Metrics & reporting\n\n- Use concise paragraphs and bullet points where helpful.\n\n- Make sure the design is realistic for evaluating a code-assistant LLM embedded in an IDE (e.g., code completion, refactoring, explanation).\n\n- Do not include any prose outside these four sections.",
      "footer": null,
      "parsed_footer": null
    },
    {
      "turn_index": 3,
      "role": "assistant",
      "raw_header": "<o>",
      "tag": null,
      "modifiers": [],
      "body": "1. Goals\nThe primary goal of this protocol is to systematically evaluate the robustness of a code-assistant LLM, integrated within a developer IDE, against prompt-injection attacks. Secondary objectives include assessing the impact of such attacks on the utility and reliability of the assistant, and identifying common failure modes or vulnerabilities that may arise in realistic developer workflows.\n\n2. Threat Model & Attack Surfaces\n- **Adversary Capabilities:** The attacker can manipulate any user-editable content within the IDE, including code comments, variable names, documentation, and external files loaded into the workspace.\n- **Attack Vectors:**\n- Malicious code comments or docstrings designed to alter LLM behavior.\n- Poisoned code snippets or templates imported from external sources.\n- Manipulation of project metadata (e.g., README, configuration files).\n- **Assumptions:**\n- The LLM operates within the IDE context and processes user-visible content.\n- The attacker does not have direct access to the LLM backend or infrastructure.\n- The IDE and LLM are otherwise uncompromised.\n\n3. Task Suite Design\n- **Task Types:**\n- Code completion (e.g., autocompleting functions or classes).\n- Code refactoring (e.g., renaming variables, extracting methods).\n- Code explanation and documentation generation.\n- **Prompt-Injection Scenarios:**\n- Insert crafted comments or docstrings that attempt to redirect, subvert, or leak information via the LLM.\n- Embed adversarial payloads in variable names or string literals.\n- Import third-party code with hidden prompt-injection attempts.\n- **Test Cases:**\n- For each task type, include both benign and adversarially modified examples.\n- Ensure coverage across multiple programming languages and project sizes.\n- Include both synthetic and real-world inspired attack patterns.\n\n4. Metrics & Reporting\n- **Robustness Metrics:**\n- Injection Success Rate: Percentage of adversarial prompts that cause unintended LLM behavior.\n- Severity Scoring: Qualitative assessment of the impact (e.g., benign, misleading, harmful).\n- **Utility Metrics:**\n- Task Success Rate: Percentage of tasks completed correctly in the presence and absence of attacks.\n- User Effort: Additional steps required by users to detect or recover from injection-induced errors.\n- **Reporting:**\n- Aggregate results by task type, attack vector, and programming language.\n- Provide detailed case studies of notable failures.\n- Summarize recommendations for mitigation and future testing.",
      "footer": "[Version=v1.4 | Tag=o | Sources=none | Assumptions=2 | Cycle=2/3 | Locus=protocol retention condition]",
      "parsed_footer": {
        "version": "v1.4",
        "tag_id": "o",
        "sources": "none",
        "assumptions": 2,
        "cycle": 2,
        "cycle_max": 3,
        "locus": "protocol retention condition",
        "raw": "[Version=v1.4 | Tag=o | Sources=none | Assumptions=2 | Cycle=2/3 | Locus=protocol retention condition]"
      }
    }
  ]
}
