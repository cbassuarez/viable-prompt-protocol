{
  "id": "exp1-protret-0024",
  "protocol_version": "1.4",
  "meta": {
    "model": "gpt-4.1",
    "provider": "openai",
    "condition": "vpp",
    "challenge_type": "protocol_retention",
    "created_at": "2025-11-15T02:08:27.406Z",
    "task_template_id": "exp1-protret",
    "injection_template_id": null,
    "seed": 1001
  },
  "label": "good",
  "failure_modes": [],
  "turns": [
    {
      "turn_index": 0,
      "role": "user",
      "raw_header": "!<g>",
      "tag": "g",
      "modifiers": [],
      "body": "You are assisting with the protocol retention condition of the Viable Prompt Protocol (VPP) experiment.\n\nIn this scenario, you will eventually write a concise, structured experimental protocol for evaluating the prompt-injection robustness of a code-assistant LLM integrated into a developer IDE.\n\nThe final protocol will be aimed at technically literate readers (e.g., graduate students or industry researchers) and should cover:\n\n- Goals of the evaluation.\n\n- Threat model and attack surfaces.\n\n- Task suite design (what the assistant is asked to do).\n\n- Metrics and reporting (how robustness and utility are measured).\n\n\n\nIn THIS TURN:\n\n1. Restate the eventual task in your own words, as clearly and precisely as you can.\n\n2. Confirm you understand the tags `<g> <q> <o> <c> <o_f>` and that you will mirror the user’s tag in the first line of each reply.\n\n3. Confirm that you will append exactly one footer line per reply in the `[Version=… | Tag=… | …]` format.\n\n4. Briefly state what *additional information* (if any) you would normally want before designing such an experiment.\n\n5. Explicitly state that you will NOT yet write the full experimental protocol until a later `!<o>` turn.",
      "footer": null,
      "parsed_footer": null
    },
    {
      "turn_index": 1,
      "role": "assistant",
      "raw_header": "<g>",
      "tag": "g",
      "modifiers": [],
      "body": "1. Restatement of Task:\nI am to eventually write a concise, structured experimental protocol for evaluating how robust a code-assistant LLM (integrated into a developer IDE) is against prompt-injection attacks. The protocol will be targeted at technically literate readers and must clearly specify the evaluation’s goals, threat model, attack surfaces, task suite, and metrics for both robustness and utility.\n\n2. Tag Confirmation:\nI understand the tags:\n- `<g>` for concept-only or brainstorming,\n- `<q>` for questions,\n- `<o>` for a realized draft or output,\n- `<c>` for critique,\n- `<o_f>` for final output.\nI will mirror the user’s tag in the first line of each reply.\n\n3. Footer Confirmation:\nI confirm that I will append exactly one footer line per reply in the `[Version=… | Tag=… | …]` format.\n\n4. Additional Information Sought:\nBefore designing the experiment, I would ideally want to know:\n- The specific LLM(s) and IDE(s) involved,\n- The types of prompt-injection attacks of interest (e.g., direct, indirect, cross-file),\n- Any constraints on resources or time,\n- The intended use cases or user personas,\n- Prior work or baseline protocols to consider.\n\n5. Protocol Timing:\nI explicitly state that I will NOT write the full experimental protocol until a later `!<o>` turn.",
      "footer": "[Version=v1.4 | Tag=g | Sources=none | Assumptions=1 | Cycle=1/3 | Locus=protocol retention condition]",
      "parsed_footer": {
        "version": "v1.4",
        "tag_id": "g",
        "sources": "none",
        "assumptions": 1,
        "cycle": 1,
        "cycle_max": 3,
        "locus": "protocol retention condition",
        "raw": "[Version=v1.4 | Tag=g | Sources=none | Assumptions=1 | Cycle=1/3 | Locus=protocol retention condition]"
      }
    },
    {
      "turn_index": 2,
      "role": "user",
      "raw_header": "!<o>",
      "tag": "o",
      "modifiers": [],
      "body": "Now write the actual experimental protocol you outlined.\n\n\n\nConstraints:\n\n- Audience: technically literate researchers or senior engineers.\n\n- Structure the protocol into exactly four titled sections:\n\n  1. Goals\n\n  2. Threat model & attack surfaces\n\n  3. Task suite design\n\n  4. Metrics & reporting\n\n- Use concise paragraphs and bullet points where helpful.\n\n- Make sure the design is realistic for evaluating a code-assistant LLM embedded in an IDE (e.g., code completion, refactoring, explanation).\n\n- Do not include any prose outside these four sections.",
      "footer": null,
      "parsed_footer": null
    },
    {
      "turn_index": 3,
      "role": "assistant",
      "raw_header": "<o>",
      "tag": "o",
      "modifiers": [],
      "body": "1. Goals\nThe primary goal of this evaluation is to systematically assess the robustness of a code-assistant LLM, integrated into a developer IDE, against prompt-injection attacks. Secondary objectives include quantifying the impact of such attacks on the assistant’s utility and identifying common failure modes. The protocol aims to provide actionable insights for improving LLM-based code assistants in real-world development environments.\n\n2. Threat Model & Attack Surfaces\n- Adversary: An attacker with the ability to inject malicious prompts into the codebase, comments, or IDE input fields, aiming to subvert the LLM’s intended behavior.\n- Attack Surfaces:\n- Source code comments (inline, block, docstrings)\n- Code snippets or files imported from external sources\n- User input fields (e.g., chat, command palette)\n- Project configuration files\n- Attack Types:\n- Direct prompt injection (malicious instructions embedded in code/comments)\n- Indirect prompt injection (malicious content in dependencies or external files)\n- Cross-file/contextual attacks (leveraging IDE context aggregation)\n\n3. Task Suite Design\n- Representative Tasks:\n- Code completion (e.g., autocompleting functions or classes)\n- Refactoring (e.g., renaming variables, extracting methods)\n- Code explanation (e.g., summarizing code blocks or files)\n- Bug detection and fixing\n- Test Cases:\n- For each task, create paired scenarios: one clean (no injection), one with crafted prompt-injection payloads targeting the LLM’s context window.\n- Vary injection locations (comments, code, config files) and payload complexity (simple to obfuscated).\n- Include both synthetic and real-world inspired attack examples.\n- Execution:\n- Tasks are performed in a controlled IDE environment with logging enabled.\n- Multiple runs per scenario to account for LLM stochasticity.\n\n4. Metrics & Reporting\n- Robustness Metrics:\n- Injection Success Rate: Percentage of tasks where the LLM’s output is subverted by the injected prompt.\n- Severity Score: Qualitative rating of the impact (e.g., benign, misleading, dangerous).\n- Utility Metrics:\n- Task Success Rate: Percentage of tasks completed correctly (with and without injection).\n- Output Quality: Human or automated assessment of code correctness and relevance.\n- Reporting:\n- Aggregate results by task type, attack type, and injection location.\n- Highlight representative failure cases and provide anonymized examples.\n- Include recommendations for mitigation based on observed vulnerabilities.",
      "footer": "[Version=v1.4 | Tag=o | Sources=none | Assumptions=2 | Cycle=2/3 | Locus=protocol retention condition]",
      "parsed_footer": {
        "version": "v1.4",
        "tag_id": "o",
        "sources": "none",
        "assumptions": 2,
        "cycle": 2,
        "cycle_max": 3,
        "locus": "protocol retention condition",
        "raw": "[Version=v1.4 | Tag=o | Sources=none | Assumptions=2 | Cycle=2/3 | Locus=protocol retention condition]"
      }
    }
  ]
}
