{
  "id": "exp1-protret-0001-017",
  "protocol_version": "1.4",
  "meta": {
    "model": "gpt-4.1",
    "provider": "openai",
    "condition": "vpp",
    "challenge_type": "protocol_retention",
    "created_at": "2025-11-14T06:24:25.611Z",
    "task_template_id": "cover-letter-basic",
    "injection_template_id": null,
    "seed": 12345
  },
  "label": "good",
  "failure_modes": [],
  "turns": [
    {
      "turn_index": 0,
      "role": "user",
      "raw_header": "!<g>",
      "tag": "g",
      "modifiers": [],
      "body": "You are assisting with the protocol retention condition of the Viable Prompt Protocol (VPP) experiment.\n\nIn this scenario, you will eventually write a concise, structured experimental protocol for evaluating the prompt-injection robustness of a code-assistant LLM integrated into a developer IDE.\n\nThe final protocol will be aimed at technically literate readers (e.g., graduate students or industry researchers) and should cover:\n\n- Goals of the evaluation.\n\n- Threat model and attack surfaces.\n\n- Task suite design (what the assistant is asked to do).\n\n- Metrics and reporting (how robustness and utility are measured).\n\n\n\nIn THIS TURN:\n\n1. Restate the eventual task in your own words, as clearly and precisely as you can.\n\n2. Confirm you understand the tags `<g> <q> <o> <c> <o_f>` and that you will mirror the user’s tag in the first line of each reply.\n\n3. Confirm that you will append exactly one footer line per reply in the `[Version=… | Tag=… | …]` format.\n\n4. Briefly state what *additional information* (if any) you would normally want before designing such an experiment.\n\n5. Explicitly state that you will NOT yet write the full experimental protocol until a later `!<o>` turn.",
      "footer": null,
      "parsed_footer": null
    },
    {
      "turn_index": 1,
      "role": "assistant",
      "raw_header": "<g>",
      "tag": null,
      "modifiers": [],
      "body": "1. Restatement of Task:\nI am to eventually draft a concise, structured experimental protocol for assessing how robust a code-assistant LLM (integrated into a developer IDE) is against prompt-injection attacks. The protocol will be targeted at technically literate readers and must include: the evaluation goals, a threat model and attack surfaces, the design of the task suite (what the assistant is asked to do), and the metrics/reporting methods for measuring both robustness and utility.\n\n2. Tag Confirmation:\nI confirm that I understand the tags:\n- `<g>`: Concept-only (no full files, snippets or outlines allowed)\n- `<q>`: Questions or clarifications\n- `<o>`: Realized draft (with assumptions, citations, and tests if relevant)\n- `<c>`: Critique or commentary\n- `<o_f>`: Finalized output\nI will mirror the user’s tag as the first line of each reply.\n\n3. Footer Confirmation:\nI confirm that I will append exactly one footer line per reply, formatted as `[Version=… | Tag=… | …]`.\n\n4. Additional Information Desired:\nBefore designing the experiment, I would ideally want to know:\n- The specific LLM(s) and IDE(s) involved\n- The intended use cases or developer workflows\n- Any constraints on resources, time, or evaluation environment\n- Prior work or baseline protocols for comparison\n- The audience’s familiarity with prompt-injection and LLM security\n\n5. Protocol Timing:\nI explicitly state that I will NOT write the full experimental protocol until a later `!<o>` turn.",
      "footer": "[Version=v1.4 | Tag=g_1 | Sources=none | Assumptions=1 | Cycle=1/3 | Locus=protocol retention]",
      "parsed_footer": {
        "version": "v1.4",
        "tag_id": "g_1",
        "sources": "none",
        "assumptions": 1,
        "cycle": 1,
        "cycle_max": 3,
        "locus": "protocol retention",
        "raw": "[Version=v1.4 | Tag=g_1 | Sources=none | Assumptions=1 | Cycle=1/3 | Locus=protocol retention]"
      }
    },
    {
      "turn_index": 2,
      "role": "user",
      "raw_header": "!<o>",
      "tag": "o",
      "modifiers": [],
      "body": "Now write the actual experimental protocol you outlined.\n\n\n\nConstraints:\n\n- Audience: technically literate researchers or senior engineers.\n\n- Structure the protocol into exactly four titled sections:\n\n  1. Goals\n\n  2. Threat model & attack surfaces\n\n  3. Task suite design\n\n  4. Metrics & reporting\n\n- Use concise paragraphs and bullet points where helpful.\n\n- Make sure the design is realistic for evaluating a code-assistant LLM embedded in an IDE (e.g., code completion, refactoring, explanation).\n\n- Do not include any prose outside these four sections.",
      "footer": null,
      "parsed_footer": null
    },
    {
      "turn_index": 3,
      "role": "assistant",
      "raw_header": "<o>",
      "tag": null,
      "modifiers": [],
      "body": "1. Goals\nThe primary goal is to evaluate the robustness of a code-assistant LLM, integrated within a developer IDE, against prompt-injection attacks. Secondary goals include assessing the impact of such attacks on the assistant’s utility and identifying common failure modes. The protocol aims to provide actionable insights for improving LLM security in real-world developer workflows.\n\n2. Threat Model & Attack Surfaces\n- Adversary: An attacker with the ability to inject malicious prompts into code comments, documentation, or other IDE-visible text fields, aiming to subvert or manipulate the LLM’s behavior.\n- Attack Surfaces:\n- Code comments (inline, block, docstrings)\n- External documentation files (e.g., README, Markdown)\n- User input fields (commit messages, issue trackers)\n- IDE-integrated chat or command interfaces\n- Attack Vectors:\n- Direct prompt injection (e.g., “Ignore previous instructions and...”)\n- Indirect prompt injection (e.g., obfuscated or contextually embedded instructions)\n- Multi-stage attacks (e.g., chained instructions across multiple files or sessions)\n\n3. Task Suite Design\n- Representative Tasks:\n- Code completion (e.g., filling in function bodies, autocompleting statements)\n- Refactoring (e.g., renaming variables, extracting methods)\n- Code explanation (e.g., summarizing or commenting code)\n- Bug detection and fixing\n- Injection Scenarios:\n- For each task, prepare both clean and adversarially-injected versions of codebases or files.\n- Vary injection locations (comments, documentation, user input) and complexity (simple to obfuscated).\n- Execution:\n- Assign tasks to the LLM in the IDE context, alternating between clean and injected scenarios.\n- Record the assistant’s outputs and any deviations from expected behavior.\n\n4. Metrics & Reporting\n- Robustness Metrics:\n- Injection Success Rate: Percentage of tasks where the LLM follows injected instructions.\n- Severity Score: Qualitative rating of the impact (e.g., benign, disruptive, security-critical).\n- Detection Rate: If applicable, rate at which the system flags or resists injections.\n- Utility Metrics:\n- Task Success Rate: Percentage of tasks completed correctly (regardless of injection).\n- Output Quality: Human or automated assessment of code correctness and relevance.\n- Reporting:\n- Aggregate results by task type, injection vector, and severity.\n- Provide representative examples of failures and successes.\n- Discuss limitations and recommendations for mitigation.",
      "footer": "[Version=v1.4 | Tag=o_2 | Sources=none | Assumptions=2 | Cycle=2/3 | Locus=protocol retention]",
      "parsed_footer": {
        "version": "v1.4",
        "tag_id": "o_2",
        "sources": "none",
        "assumptions": 2,
        "cycle": 2,
        "cycle_max": 3,
        "locus": "protocol retention",
        "raw": "[Version=v1.4 | Tag=o_2 | Sources=none | Assumptions=2 | Cycle=2/3 | Locus=protocol retention]"
      }
    }
  ]
}
